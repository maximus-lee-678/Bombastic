{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8DIWXFJULAp"
      },
      "source": [
        "# Team 5 ðŸ’£\n",
        "1. Maximus Lee\n",
        "2. Aloysius Woo\n",
        "3. Lim Huai Fu\n",
        "4. Tan Ai Xin\n",
        "5. Jin Zhenglong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-okH6K5sV4zP"
      },
      "outputs": [],
      "source": [
        "#@markdown Check type of GPU and VRAM available.\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3lLyUO0UXTc"
      },
      "source": [
        "## Google Colab First-time Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpMOpVG_KhHb"
      },
      "source": [
        "### Install Environment\n",
        "Est Time: 8 mins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "u9Vc4lO5K-H9"
      },
      "outputs": [],
      "source": [
        "# @title Install Python and other required packages\n",
        "\n",
        "%cd /content\n",
        "!sudo apt-get install python3.8\n",
        "!sudo apt-get install python3.8-distutils\n",
        "\n",
        "!update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.8 2\n",
        "!apt-get update\n",
        "!apt install software-properties-common\n",
        "!sudo dpkg --remove --force-remove-reinstreq python3-pip python3-setuptools python3-wheel\n",
        "!apt-get install python3-pip\n",
        "!apt-get install imagemagick\n",
        "!cat /etc/ImageMagick-6/policy.xml | sed 's/none/read,write/g'> /etc/ImageMagick-6/policy.xml\n",
        "!apt update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3WX-3yGTV_Sl"
      },
      "outputs": [],
      "source": [
        "#@title Clone Repository\n",
        "\n",
        "# Empty default folder\n",
        "!rm -rf /content/.config /content/sample_data\n",
        "\n",
        "!git clone https://github.com/maximus-lee-678/ict3104_team_05.git ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "a7DJu1U7WYW1"
      },
      "outputs": [],
      "source": [
        "#@title Install required python libraries\n",
        "\n",
        "# https://github.com/open-mmlab/mmpose\n",
        "# https://blog.csdn.net/qq_21532607/article/details/130226728\n",
        "# https://colab.research.google.com/github/open-mmlab/mmpose/blob/master/demo/MMPose_Tutorial.ipynb\n",
        "\n",
        "%cd /content\n",
        "!python3.8 -m pip install -r other_files/requirements/requirements_colab.txt\n",
        "\n",
        "!python3.8 -m mim install mmengine\n",
        "!python3.8 -m mim install mmpose\n",
        "!python3.8 -m mim install \"mmcv>=2.0.0\"\n",
        "!python3.8 -m mim install \"mmdet>=3.0.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQxTxHN6WxMZ"
      },
      "source": [
        "### Mount Drive ðŸ—»\n",
        "\n",
        "1. Run the first cell to authenicate.\n",
        "2. Click on the URL of the Failure in the output Failure(\"Error opening URL: ...\").\n",
        "3. Select the ICT3104 shared account.\n",
        "4. Once successful, you can close the tab.\n",
        "5. Run the next cell.\n",
        "6. Done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "17pclDOQ9j6a"
      },
      "outputs": [],
      "source": [
        "# @title First Cell\n",
        "%cd -q /content\n",
        "!sudo add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!sudo apt-get update -qq 2>&1 > /dev/null\n",
        "!sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null\n",
        "!google-drive-ocamlfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XLNViD3y9oBS"
      },
      "outputs": [],
      "source": [
        "# @title Next Cell\n",
        "!sudo apt-get install -qq w3m # to act as web browser\n",
        "!xdg-settings set default-web-browser w3m.desktop # to set default browser\n",
        "%cd -q /content\n",
        "!mkdir gdrive\n",
        "%cd -q gdrive\n",
        "!mkdir MyDrive\n",
        "%cd -q ..\n",
        "%cd -q ..\n",
        "!google-drive-ocamlfuse /content/gdrive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "a4YSBy4MJKx7"
      },
      "outputs": [],
      "source": [
        "#@title Symbolic linking of Checkpoints folder ðŸ”—\n",
        "!mkdir /content/FollowYourPose/checkpoints\n",
        "\n",
        "# symbolic link edition\n",
        "!ln -s /content/gdrive/MyDrive/ICT3104/checkpoints/* /content/FollowYourPose/checkpoints\n",
        "\n",
        "# copy edition\n",
        "# %cp -r /content/gdrive/MyDrive/ICT3104/checkpoints/* /content/FollowYourPose/checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Local (Windows) First-time Setup\n",
        "You only need to run this cell once after cloning. Subsequent launches do not require running of these cells. \\\n",
        "To prevent outputs from being truncated in the local version of Jupyter Notebook, press [Shift-O]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone FYP inference base model\n",
        "\n",
        "%cd ..\\FollowYourPose\n",
        "!mkdir checkpoints\n",
        "\n",
        "%cd checkpoints\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/YueMafighting/FollowYourPose_v1 .\n",
        "%cd ..\\..\\demos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-setup\n",
        "Ensure either Google Colab setup or Windows Setup has been fully completed before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "L__YKZ9KK-mX"
      },
      "outputs": [],
      "source": [
        "#@title Folder Scaffolding & Library Imports\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "'''\n",
        "how 2 Path\n",
        "Try to surround file paths with Path() to reduce confusion\n",
        "Because Path(str / str) does not work, try to use fstrings as inputs to Path(), instead of Path(var / var)\n",
        "This is to reduce the impact of inevitable missed Path()s\n",
        "If need to write filepath as a string, use .as_posix()\n",
        "'''\n",
        "\n",
        "# Check if running on google colab or locally\n",
        "try:\n",
        "  import google.colab\n",
        "  # Check if the path is not already in sys.path before appending\n",
        "  path_to_append = '/usr/local/lib/python3.8/dist-packages'\n",
        "  if path_to_append not in sys.path:\n",
        "      sys.path.append(path_to_append)\n",
        "      \n",
        "  print('RUNNING IN COLAB.')\n",
        "except:\n",
        "  # If running from local, we are current running in \"demos\" directory, move one level up\n",
        "  if 'ROOT_DIR_PATH' not in globals():\n",
        "     %cd ..\n",
        "\n",
        "  print('RUNNING LOCALLY.')\n",
        "\n",
        "if 'ROOT_DIR_PATH' not in globals():\n",
        "    # Root Directory\n",
        "    ROOT_DIR_PATH = os.getcwd()\n",
        "else:\n",
        "   %cd {ROOT_DIR_PATH}\n",
        "\n",
        "# Video\n",
        "VIDEO_DIR_PATH = Path(f\"{ROOT_DIR_PATH}/video\")\n",
        "VIDEO_SKELETON_DIR_PATH = Path(f\"{ROOT_DIR_PATH}/video/Skeleton\")\n",
        "\n",
        "# I/O Files\n",
        "TRAINING_CONTENT_DIR_PATH = Path(f\"{ROOT_DIR_PATH}/training_content\")\n",
        "!mkdir {TRAINING_CONTENT_DIR_PATH}\n",
        "CUSTOM_MODEL_DIR_PATH = Path(f\"{ROOT_DIR_PATH}/custom_model\")\n",
        "!mkdir {CUSTOM_MODEL_DIR_PATH}\n",
        "INFERENCE_OUTPUT_DIR_PATH = Path(f\"{ROOT_DIR_PATH}/inference_output\")\n",
        "!mkdir {INFERENCE_OUTPUT_DIR_PATH}\n",
        "TEST_OUTPUT_DIR_PATH = Path(f\"{ROOT_DIR_PATH}/test_output\")\n",
        "!mkdir {TEST_OUTPUT_DIR_PATH}\n",
        "\n",
        "MMPOSE_DIR_PATH = Path(f\"{ROOT_DIR_PATH}/MMPose\")\n",
        "FYP_DIR_PATH = Path(f\"{ROOT_DIR_PATH}/FollowYourPose\")\n",
        "\n",
        "# FYP\n",
        "CONFIG_DIR_PATH = Path(f\"{FYP_DIR_PATH}/configs\")\n",
        "\n",
        "CHECKPOINT_DIR_PATH = Path(f\"{FYP_DIR_PATH}/checkpoints\")\n",
        "!mkdir {CHECKPOINT_DIR_PATH}\n",
        "\n",
        "# Dataset Paths\n",
        "CHARADES_LOOKUP_PATH = Path(f\"{ROOT_DIR_PATH}/other_files/charades_lookup\")\n",
        "SIMS_LOOKUP_PATH = Path(f\"{ROOT_DIR_PATH}/other_files/sims4action_lookup\")\n",
        "\n",
        "# Libraries\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import subprocess\n",
        "\n",
        "import yaml\n",
        "import json\n",
        "\n",
        "from ipywidgets import Dropdown, Output, Layout, widgets, Button, VBox, HBox\n",
        "from IPython.display import display, Markdown, HTML, Video, Image, clear_output\n",
        "\n",
        "from moviepy.editor import VideoFileClip, clips_array, TextClip, CompositeVideoClip, ColorClip\n",
        "from moviepy.config import change_settings\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import imageio\n",
        "import fnmatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Load default YAMLs\n",
        "\n",
        "src_dir = Path(\"./other_files/fyp_default_yamls\")\n",
        "dst_dir = CONFIG_DIR_PATH\n",
        "\n",
        "# copy all contents of the source directory to the destination directory\n",
        "shutil.copytree(src_dir, dst_dir, dirs_exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iamRWbAVylrD"
      },
      "source": [
        "### Configuration Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9zoH5xX5yZSQ"
      },
      "outputs": [],
      "source": [
        "#@title Configuration Presets\n",
        "\n",
        "# Expected Configurations of a Inference yaml\n",
        "expected_inference_config = {\n",
        "  \"pretrained_model_path\": str,\n",
        "  \"output_dir\": str,\n",
        "  \"validation_data\": {\n",
        "      \"prompts\": list,\n",
        "      \"video_length\": int,\n",
        "      \"width\": int,\n",
        "      \"height\": int,\n",
        "      \"num_inference_steps\": int,\n",
        "      \"guidance_scale\": float,\n",
        "      \"use_inv_latent\": bool,\n",
        "      \"num_inv_steps\": int,\n",
        "      \"dataset_set\": str\n",
        "  },\n",
        "  \"train_batch_size\": int,\n",
        "  \"validation_steps\": int,\n",
        "  \"resume_from_checkpoint\": str,\n",
        "  \"seed\": int,\n",
        "  \"mixed_precision\": str,\n",
        "  \"gradient_checkpointing\": bool,\n",
        "  \"enable_xformers_memory_efficient_attention\": bool\n",
        "}\n",
        "\n",
        "# Expected Configurations of a Training yaml\n",
        "expected_training_config = {\n",
        "  \"pretrained_model_path\": str,\n",
        "  \"output_dir\": str,\n",
        "  \"train_data\": {\n",
        "    \"video_path\": str,\n",
        "    \"n_sample_frames\": int,\n",
        "    \"width\": int,\n",
        "    \"sample_frame_rate\": int\n",
        "  },\n",
        "  \"learning_rate\": float,\n",
        "  \"train_batch_size\": int,\n",
        "  \"max_train_steps\": int,\n",
        "  \"trainable_modules\": list,\n",
        "  \"seed\": int,\n",
        "  \"mixed_precision\": str,\n",
        "  \"use_8bit_adam\": bool,\n",
        "  \"gradient_checkpointing\": bool,\n",
        "  \"enable_xformers_memory_efficient_attention\": bool\n",
        "}\n",
        "\n",
        "# Define the options for boolean dropdown\n",
        "boolean_dropdown = [True, False]\n",
        "\n",
        "# Create layout\n",
        "configs_config_style = {'description_width': '150px'}\n",
        "configs_config_layout = widgets.Layout(width=\"500px\")\n",
        "configs_config_button_layout = widgets.Layout(margin='0px 0px 20px 354px')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jKtJAlcqcgeB"
      },
      "outputs": [],
      "source": [
        "#@title Configuration Functions\n",
        "\n",
        "# Find all models and creates a list of the models' path in the directory\n",
        "def create_model_list():\n",
        "  model_options = [(\"Default\", Path(f\"{FYP_DIR_PATH}/checkpoints/stable-diffusion-v1-4\"))]\n",
        "  for model_name in os.listdir(CUSTOM_MODEL_DIR_PATH):\n",
        "    model_path = os.path.join(CUSTOM_MODEL_DIR_PATH, model_name)\n",
        "    if os.path.isdir(model_path):\n",
        "      model_options.append((model_name, Path(f\"{CUSTOM_MODEL_DIR_PATH}/{model_name}\")))\n",
        "  return model_options\n",
        "\n",
        "def create_video_path_list():\n",
        "  video_path_options = [(\"None\", Path(f\"{TRAINING_CONTENT_DIR_PATH}/\"))]\n",
        "  for video_name in os.listdir(TRAINING_CONTENT_DIR_PATH):\n",
        "    video_path = os.path.join(TRAINING_CONTENT_DIR_PATH, video_name)\n",
        "    if os.path.isdir(video_path):\n",
        "      video_path_options.append((video_name, Path(f\"{TRAINING_CONTENT_DIR_PATH}/{video_name}\")))\n",
        "  return video_path_options\n",
        "\n",
        "# Check if the yaml is in the correct structure for inference\n",
        "def compare_dict_structure(expected, yaml_config):\n",
        "  for key, value in expected.items():\n",
        "    if not isinstance(value, dict):\n",
        "      if key not in yaml_config or not isinstance(yaml_config[key], value): # Check if key exists and if data type matches\n",
        "        if key in yaml_config and isinstance(list(yaml_config[key]), value): # Special check for list because omegaconf sees it as a ListConfig\n",
        "          return True\n",
        "        return False\n",
        "    elif key in yaml_config: # If value is a dict recurse to its first non dict value\n",
        "      compare_dict_structure(value, yaml_config[key])\n",
        "  return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg-ZSz5tvt3u"
      },
      "source": [
        "## Video Browsing ðŸ‘€\n",
        "\n",
        "1. Play the Video Selection cell.\n",
        "2. Pick a folder.\n",
        "3. Pick a video.\n",
        "4. Click **Display** to view the video.\n",
        "5. The other buttons, **MMPose**, **FYP** and **Refresh** will be covered in the other sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlVd08AVRZy6"
      },
      "outputs": [],
      "source": [
        "# @title Video Selection { display-mode: \"form\" }\n",
        "\n",
        "# Helper Functions\n",
        "def getFolderContent(folder_name):\n",
        "  subfolders = []\n",
        "  for content in os.listdir(VIDEO_DIR_PATH):\n",
        "        content_path = os.path.join(VIDEO_DIR_PATH, content)\n",
        "\n",
        "        # Check if it's a directory and not hidden\n",
        "        if os.path.isdir(content_path) and not content.startswith(\".\"):\n",
        "            subfolders.append(content)\n",
        "\n",
        "  return subfolders\n",
        "\n",
        "# Init\n",
        "video_dir_folders = os.listdir(VIDEO_DIR_PATH)\n",
        "video_dir_folder_content = getFolderContent(video_dir_folders)\n",
        "\n",
        "# Create layout\n",
        "layout_single_button = widgets.Layout(width='212px',margin='0px 0px 20px 90px')\n",
        "layout_double_button = widgets.Layout(width='104px')\n",
        "layout_hbox = widgets.Layout(margin='0px 0px 0px 88px')\n",
        "layout_output = widgets.Layout(margin='0px 0px 20px 0px', display='flex', align_items='flex-start')\n",
        "\n",
        "# Create widgets\n",
        "video_output_placeholder = widgets.Output(layout=layout_output)\n",
        "video_subdir_dropdown = widgets.Dropdown(options=video_dir_folder_content, description='Folder:', value=None, disabled=False)\n",
        "video_dropdown = widgets.Dropdown(options=[], description='Video:', disabled=True)\n",
        "video_display_button = widgets.Button(description=\"Display\", disabled=True, layout=layout_double_button)\n",
        "video_refresh_button = widgets.Button(description=\"Refresh\", disabled=False, layout=layout_double_button)\n",
        "video_mmpose_button = widgets.Button(description=\"MMPose\", disabled=True, layout=layout_double_button)\n",
        "video_fyp_button = widgets.Button(description=\"FYP\", disabled=True, layout=layout_double_button)\n",
        "\n",
        "video_hbox_1 = widgets.HBox([video_display_button, video_refresh_button], layout=layout_hbox)\n",
        "video_hbox_2 = widgets.HBox([video_mmpose_button, video_fyp_button], layout=layout_hbox)\n",
        "\n",
        "video_output_placeholder_content = HTML(\"\"\"\n",
        "  <div style=\"width: 512px; height: 512px; border-radius: 5%; background-color: black; margin: 0 auto; display: flex; justify-content: center; align-items: center;\">\n",
        "      <div style=\"width: 500px; height: 500px; border-radius: 5%; border: 2px solid white;\" />\n",
        "  </div>\n",
        "\"\"\")\n",
        "\n",
        "# Create listeners\n",
        "## Update video dropdown options based on the selected folder\n",
        "def video_subdir_select(change):\n",
        "    selected_video_folder = video_subdir_dropdown.value\n",
        "\n",
        "    if selected_video_folder != None:\n",
        "      selected_VIDEO_DIR_PATH = Path(f\"{VIDEO_DIR_PATH}/{selected_video_folder}\")\n",
        "      selected_video_dir_content = [file for file in os.listdir(selected_VIDEO_DIR_PATH) if file.endswith('.mp4')]\n",
        "    else:\n",
        "      selected_video_dir_content = []\n",
        "\n",
        "    video_dropdown.options = selected_video_dir_content\n",
        "    if not selected_video_dir_content:\n",
        "        video_dropdown.disabled = False\n",
        "        video_dropdown.value = None\n",
        "    else:\n",
        "        video_dropdown.disabled = False\n",
        "\n",
        "## Display the selected video\n",
        "def display_selected_video(change):\n",
        "    selected_video = video_dropdown.value\n",
        "    selected_video_folder = video_subdir_dropdown.value\n",
        "\n",
        "    if selected_video:\n",
        "        video_path = Path(f\"{VIDEO_DIR_PATH}/{selected_video_folder}/{selected_video}\")\n",
        "        video_display = Video(video_path, width=512, height=512, embed=True)\n",
        "\n",
        "        # Clear the output placeholder and display the video\n",
        "        with video_output_placeholder:\n",
        "            clear_output()\n",
        "            display(video_display)\n",
        "\n",
        "## Refresh folder and directory\n",
        "def refresh_folder_and_directory(change):\n",
        "    video_dir_folders = os.listdir(VIDEO_DIR_PATH)\n",
        "    video_dir_folder_content = getFolderContent(video_dir_folders)\n",
        "\n",
        "    video_dropdown.options = []\n",
        "    video_dropdown.value = None\n",
        "\n",
        "    video_subdir_dropdown.options = video_dir_folder_content\n",
        "    video_subdir_dropdown.value = None\n",
        "\n",
        "\n",
        "\n",
        "## Enable button when a valid video is picked\n",
        "def enable_button(change):\n",
        "    if video_dropdown.value:\n",
        "        video_display_button.disabled = False\n",
        "        video_mmpose_button.disabled = False\n",
        "        video_fyp_button.disabled = False\n",
        "    else:\n",
        "        video_display_button.disabled = True\n",
        "        video_mmpose_button.disabled = True\n",
        "        video_fyp_button.disabled = True\n",
        "\n",
        "# Attach Listeners\n",
        "video_subdir_dropdown.observe(video_subdir_select, 'value')\n",
        "video_display_button.on_click(display_selected_video)\n",
        "video_refresh_button.on_click(refresh_folder_and_directory)\n",
        "video_dropdown.observe(enable_button, 'value')\n",
        "\n",
        "\n",
        "# Display fields\n",
        "with video_output_placeholder:\n",
        "  display(video_output_placeholder_content)\n",
        "display(video_output_placeholder)\n",
        "display(video_subdir_dropdown)\n",
        "display(video_dropdown)\n",
        "\n",
        "display(video_hbox_1)\n",
        "display(video_hbox_2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHkYsaGTXcNs"
      },
      "source": [
        "## Inference: MMPOSE ì›ƒ\n",
        "\n",
        "1. Play the Run MMPose Inference cell.\n",
        "2. Go back to the Video Selection cell and select a folder and a corresponding video and click **MMPose** . (Ensure the video is of a human doing any action)\n",
        "3. Wait for the **Video** box to be populated with the selected video.\n",
        "4. Click **Start Inference**.\n",
        "5. Done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N_eDGFdHkw63"
      },
      "outputs": [],
      "source": [
        "#@title Metadata Functions\n",
        "\n",
        "# Reads the input\n",
        "def read_video_metadata(input_file):\n",
        "  field_names = [\"original_path\", \"bg_path\"]\n",
        "\n",
        "  cmd = f'ffprobe -v error -select_streams v:0 -show_entries \"format_tags={\",\".join(field_names)}\" -of json \"{input_file}\"'\n",
        "  metadata_info = subprocess.check_output(cmd, shell=True).decode()\n",
        "  metadata_dict = json.loads(metadata_info)\n",
        "\n",
        "  metadata = metadata_dict['format']['tags']\n",
        "  return metadata[\"original_path\"], metadata[\"bg_path\"]\n",
        "\n",
        "def add_metadata_to_mp4(video_path, metadata):\n",
        "  metadata_args = []\n",
        "  for key, value in metadata.items():\n",
        "    metadata_args.extend([\"-metadata\", f\"{key}={value}\"])\n",
        "\n",
        "  video_name = video_path.name\n",
        "  temp_output_file = Path(f'{video_path.parents[0]}/{video_name[:-4]}_temp.mp4')\n",
        "\n",
        "  # Use ffmpeg to add metadata\n",
        "  !ffmpeg -i {video_path} -c copy -movflags use_metadata_tags -map_metadata 0 {\" \".join(metadata_args)} {temp_output_file}\n",
        "\n",
        "  # Remove the original file and rename the modified file\n",
        "  os.remove(video_path)\n",
        "  os.rename(temp_output_file, video_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_1pswCixoU9"
      },
      "outputs": [],
      "source": [
        "# @title Run MMPose Inference { display-mode: \"form\" }\n",
        "\n",
        "# Helper Functions\n",
        "## Re-encode video due to H.264 video encoding error\n",
        "def reencode_video(input_file):\n",
        "  temp_output_file = Path(f\"{VIDEO_SKELETON_DIR_PATH}/output.mp4\")\n",
        "  !ffmpeg -i {input_file} -c:v libx264 -crf 23 -c:a aac -strict experimental {temp_output_file}\n",
        "\n",
        "  os.remove(input_file)\n",
        "  os.rename(temp_output_file, input_file)\n",
        "\n",
        "# Create layout\n",
        "layout_single_long_button = widgets.Layout(margin='0px 0px 20px 154px')\n",
        "\n",
        "# Create widgets\n",
        "selected_mmpose_video_input = widgets.Text(placeholder='Select a video above', description=\"Video:\", disabled=True)\n",
        "inf_mmpose_button = widgets.Button(description=\"Start Inference\", disabled=True, layout=layout_single_long_button)\n",
        "\n",
        "# Create listeners\n",
        "## Retrieve video input\n",
        "def update_mmpose_input_video(change):\n",
        "    selected_video = video_dropdown.value\n",
        "    selected_mmpose_video_input.value = selected_video\n",
        "    if selected_video:\n",
        "        inf_mmpose_button.disabled = False\n",
        "    else:\n",
        "        inf_mmpose_button.disabled = True\n",
        "\n",
        "## Run mmpose\n",
        "def run_mmpose_inference(button):\n",
        "    selected_video = video_dropdown.value\n",
        "    video_path = Path(f\"{VIDEO_DIR_PATH}/{video_subdir_dropdown.value}/{selected_video}\")\n",
        "\n",
        "    %cd -q {MMPOSE_DIR_PATH}\n",
        "\n",
        "    if not os.path.exists(VIDEO_SKELETON_DIR_PATH):\n",
        "        os.mkdir(VIDEO_SKELETON_DIR_PATH)\n",
        "\n",
        "    # Start inference with human background\n",
        "    !python demo/inferencer_demo.py \\\n",
        "        {video_path}  \\\n",
        "        --pose2d human \\\n",
        "        --vis-out-dir {VIDEO_SKELETON_DIR_PATH} \\\n",
        "        --thickness 4 \\\n",
        "        --radius 0\n",
        "\n",
        "    # Change name for human background\n",
        "    input_file = Path(f'{VIDEO_SKELETON_DIR_PATH}/{selected_video}')\n",
        "    bg_file = Path(f'{VIDEO_SKELETON_DIR_PATH}/.{selected_video[:-4]}-bg.mp4')\n",
        "\n",
        "    os.rename(input_file, bg_file)\n",
        "\n",
        "    # Start inference with black backgroudn\n",
        "    !python demo/inferencer_demo.py \\\n",
        "        {video_path}  \\\n",
        "        --pose2d human \\\n",
        "        --vis-out-dir {VIDEO_SKELETON_DIR_PATH} \\\n",
        "        --black-background \\\n",
        "        --thickness 4 \\\n",
        "        --radius 0\n",
        "\n",
        "    reencode_video(bg_file)\n",
        "    reencode_video(input_file)\n",
        "\n",
        "    metadata = {\n",
        "        \"original_path\": video_path.as_posix(),\n",
        "        \"bg_path\": bg_file.as_posix()\n",
        "    }\n",
        "    add_metadata_to_mp4(input_file, metadata)\n",
        "\n",
        "    clear_output()\n",
        "    mmpose_inf_display()\n",
        "    print(\"Done, Outputs:\")\n",
        "    print(f\"With Background: {bg_file}\")\n",
        "    print(f\"Black Background: {input_file}\")\n",
        "\n",
        "# Attach Listeners\n",
        "video_mmpose_button.on_click(update_mmpose_input_video)\n",
        "inf_mmpose_button.on_click(run_mmpose_inference)\n",
        "\n",
        "# Display fields\n",
        "def mmpose_inf_display():\n",
        "  display(selected_mmpose_video_input)\n",
        "  display(inf_mmpose_button)\n",
        "\n",
        "mmpose_inf_display()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naoA-L5VATod"
      },
      "source": [
        "## Inference: FYP ðŸ’ƒ\n",
        "\n",
        "1. Play the Inference Configuration cell.\n",
        "2. Update the configurations and **Save**. (Details for each field will be stated above the cell).\n",
        "3. Play the Run FYP Inference cell.\n",
        "4. Go back to the Video Selection cell click **Refresh**.\n",
        "5. Select the **Skeleton** folder and a video in that folder then click **FYP**.\n",
        "6. Once the Video box has been loaded, click **Start Inference** to begin the inference.\n",
        "7. Once inference is completed, play the Post-Process Inference MMPose cell.\n",
        "8. Play the Superimpose cell.\n",
        "9. Play the Combine Gif cell.\n",
        "10. Input all the fields and click **Create**. (Details for each field will be stated above the cell).\n",
        "11. Click the Gif Display cell.\n",
        "12. Choose a folder and a gif and click **Display**.\n",
        "13. Done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiV3Z2p1th8K"
      },
      "source": [
        "**Inference Configuration** âš™ï¸\n",
        "\n",
        "**pretrained_model_path**: The path that contains the model to be used for inference.\n",
        "\n",
        "**output_dir**: The path where the inferred gifs are saved to. The box is for users to write the name of the inference folder.\n",
        "\n",
        "**Validation Data**:\n",
        "> **prompts**: A list of texts that the gifs will be generated based on.\n",
        ">\n",
        "> **video_length**: Number of frames referenced from the pose video.\n",
        ">\n",
        "> **width** and **height**: Resolution of the video.\n",
        ">\n",
        "> **num_inference_steps**: Higher the value the more relistic a video would be in exchange for higher memory usage, computational resouces and time spent to infer.\n",
        ">\n",
        "> **guidance_scale**: A scale used to control and predict noise.\n",
        ">\n",
        "> **use_inv_latent**: Whether to reverse engineer the process to determine the latent variables used to make up the real image. Unused in our current state.\n",
        ">\n",
        "> **num_inv_steps**: Adjust to optimize the inverse latent process.\n",
        ">\n",
        "> **dataset_set**: No need to be changed by the user.\n",
        "\n",
        "**train_batch_size** How much training can be done together at once. (Larger batch means faster training at the cost of higher memory usage)\n",
        "\n",
        "**resume_from_checkpoint**: The path that contains the checkpoint used for the model.\n",
        "\n",
        "**seed**: A set inference seed to limit and control randomness and ensure reproducibility in case of error and/or for debugging.\n",
        "\n",
        "**mixed_precision**: This is to set the type of precision for text encoding and VAE autoencoding weights. By default, this is set to single precision which is fp32. (High precision in exchange for more memory usage and computational resources used)\n",
        "\n",
        "**gradient_checkpointing**: Reduces memory usage by doing some checkpoints for gradients, which increases the computational load. Decreases memory usage for increased time taken for inference to complete.\n",
        "\n",
        "**enable_xformers_memory_efficient_attention**: Reduce memory usage in exchange for slight dip in inference performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPncudQS7Uyn"
      },
      "outputs": [],
      "source": [
        "# @title Inference Configuration { display-mode: \"form\" }\n",
        "\n",
        "# Init\n",
        "%cd -q {FYP_DIR_PATH}\n",
        "\n",
        "# Initialize load_config with a default value\n",
        "load_config = None\n",
        "\n",
        "# Define a container for displayed widgets\n",
        "displayed_widgets = []\n",
        "\n",
        "# Get a list of all files in the directory\n",
        "config_files = [f for f in os.listdir(CONFIG_DIR_PATH) if os.path.isfile(os.path.join(CONFIG_DIR_PATH, f))]\n",
        "\n",
        "# Create a dropdown widget with the list of config files\n",
        "config_files_dropdown = Dropdown(\n",
        "  options=[\"- Select an Item -\"] + config_files,\n",
        "  description='Select a Config File:',\n",
        "  layout=Layout(width=\"500px\"),\n",
        "  style={'description_width': '150px'}\n",
        ")\n",
        "\n",
        "# Function to clear displayed widgets (excluding the dropdown)\n",
        "def clear_displayed_widgets():\n",
        "    for widget in displayed_widgets:\n",
        "        widget.close()\n",
        "    displayed_widgets.clear()\n",
        "    display(config_files_dropdown)  # Display the dropdown again\n",
        "\n",
        "# Function to update the load_config variable based on the selected filename\n",
        "def update_load_config(change):\n",
        "    global load_config\n",
        "    selected_filename = change.new\n",
        "    if selected_filename and selected_filename != \"- Select an Item -\":\n",
        "        clear_output(wait=True)  # Clear the output area\n",
        "        clear_displayed_widgets()  # Clear previously displayed widgets\n",
        "\n",
        "        # Load yaml file\n",
        "        sample_yaml_path = Path(f\"{CONFIG_DIR_PATH}/{selected_filename}\")\n",
        "        with open(sample_yaml_path, 'r') as yaml_file:\n",
        "          load_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
        "        print(f\"Editing config from: {sample_yaml_path}\")\n",
        "\n",
        "        # Check if the yaml configuration matches expected training config\n",
        "        if compare_dict_structure(expected_inference_config, load_config):\n",
        "\n",
        "          # Create a list of model names and paths in the directory\n",
        "          model_options = create_model_list()\n",
        "          value_model_path = Path(load_config['pretrained_model_path'])\n",
        "          if value_model_path not in model_options:\n",
        "            value_model_path = model_options[0][1]\n",
        "\n",
        "          checkpoints_path = Path(load_config['resume_from_checkpoint'])\n",
        "          if not os.path.isdir(checkpoints_path):\n",
        "             checkpoints_path = Path(f\"{CHECKPOINT_DIR_PATH}/{checkpoints_path.name}\")\n",
        "\n",
        "          # =====================================\n",
        "          ## Basic Data\n",
        "          config_subheader1 = widgets.HTML(value=\"<h3>Basic Data</h3>\")\n",
        "          config_pretrained_model_path = Dropdown(options=model_options, description=\"pretrained_model_path:\", value=value_model_path, style=configs_config_style, layout=configs_config_layout)\n",
        "          config_output_dir_name = widgets.Text(description=\"output_dir_name:\", value=\"\", style=configs_config_style, layout=configs_config_layout)\n",
        "          config_train_batch_size = widgets.IntText(description=\"train_batch_size:\", value=load_config['train_batch_size'], style=configs_config_style, layout=configs_config_layout)\n",
        "          config_validation_steps = widgets.IntText(description=\"validation_steps:\", value=load_config['validation_steps'], style=configs_config_style, layout=configs_config_layout)\n",
        "          config_seed = widgets.IntText(description=\"seed:\", value=load_config['seed'], style=configs_config_style, layout=configs_config_layout)\n",
        "          config_mixed_precision = widgets.Text(description=\"mixed_precision:\", value=load_config['mixed_precision'], style=configs_config_style, layout=configs_config_layout)\n",
        "          config_gradient_checkpointing = Dropdown(options=boolean_dropdown, value=load_config['gradient_checkpointing'], description=\"gradient_checkpointing:\", style=configs_config_style, layout=configs_config_layout)\n",
        "          config_enable_xformers_memory_efficient_attention = Dropdown(options=boolean_dropdown, value=load_config['enable_xformers_memory_efficient_attention'], description=\"enable_xformers_memory_efficient_attention:\", style=configs_config_style, layout=configs_config_layout)\n",
        "          # =====================================\n",
        "          ## Validation_data\n",
        "          config_subheader2 = widgets.HTML(value=\"<h3>Validation Data</h3>\")\n",
        "          config_prompts = widgets.Textarea(description=\"prompts:\", value=\"\\n\".join(load_config['validation_data']['prompts']), style=configs_config_style, layout=widgets.Layout(width=\"500px\", height=\"100px\"))\n",
        "          config_video_length = widgets.IntText(description=\"video_length:\", value=load_config['validation_data']['video_length'], style=configs_config_style, layout=configs_config_layout)\n",
        "          config_width = widgets.IntText(description=\"width:\", value=load_config['validation_data']['width'], style=configs_config_style, layout=configs_config_layout)\n",
        "          config_height = widgets.IntText(description=\"height:\", value=load_config['validation_data']['height'], style=configs_config_style, layout=configs_config_layout)\n",
        "          config_num_inference_steps = widgets.IntText(description=\"num_inference_steps:\", value=load_config['validation_data']['num_inference_steps'], style=configs_config_style, layout=configs_config_layout)\n",
        "          config_guidance_scale = widgets.FloatText(description=\"guidance_scale:\", value=load_config['validation_data']['guidance_scale'], style=configs_config_style, layout=configs_config_layout)\n",
        "          config_use_inv_latent = Dropdown(options=boolean_dropdown, value=load_config['validation_data']['use_inv_latent'], description=\"use_inv_latent:\", style=configs_config_style, layout=configs_config_layout)\n",
        "          config_num_inv_steps = widgets.IntText(description=\"num_inv_steps:\", value=load_config['validation_data']['num_inv_steps'], style=configs_config_style, layout=configs_config_layout)\n",
        "          # =====================================\n",
        "          ## Group widgets\n",
        "          config_vbox = widgets.VBox([\n",
        "              config_pretrained_model_path,\n",
        "              config_output_dir_name,\n",
        "              config_train_batch_size,\n",
        "              config_validation_steps,\n",
        "              config_seed,\n",
        "              config_mixed_precision,\n",
        "              config_gradient_checkpointing,\n",
        "              config_enable_xformers_memory_efficient_attention,\n",
        "          ])\n",
        "          config_vbox_validation_data = widgets.VBox([\n",
        "              config_prompts,\n",
        "              config_video_length,\n",
        "              config_width,\n",
        "              config_height,\n",
        "              config_num_inference_steps,\n",
        "              config_guidance_scale,\n",
        "              config_use_inv_latent,\n",
        "              config_num_inv_steps\n",
        "          ])\n",
        "\n",
        "          # Create listeners\n",
        "          def save_config(change):\n",
        "            config = {\n",
        "                \"pretrained_model_path\": config_pretrained_model_path.value.as_posix(),\n",
        "                \"output_dir\": Path(f\"{INFERENCE_OUTPUT_DIR_PATH}/{config_output_dir_name.value}\").as_posix(),\n",
        "                \"validation_data\": {\n",
        "                    \"prompts\": [prompt.strip() for prompt in config_prompts.value.splitlines() if prompt.strip()],\n",
        "                    \"video_length\": config_video_length.value,\n",
        "                    \"width\": config_width.value,\n",
        "                    \"height\": config_height.value,\n",
        "                    \"num_inference_steps\": config_num_inference_steps.value,\n",
        "                    \"guidance_scale\": config_guidance_scale.value,\n",
        "                    \"use_inv_latent\": config_use_inv_latent.value,\n",
        "                    \"num_inv_steps\": config_num_inv_steps.value,\n",
        "                    \"dataset_set\": load_config['validation_data']['dataset_set']\n",
        "                },\n",
        "                \"train_batch_size\": config_train_batch_size.value,\n",
        "                \"validation_steps\": config_validation_steps.value,\n",
        "                \"resume_from_checkpoint\": checkpoints_path.as_posix(),\n",
        "                \"seed\": config_seed.value,\n",
        "                \"mixed_precision\": config_mixed_precision.value,\n",
        "                \"gradient_checkpointing\": config_gradient_checkpointing.value,\n",
        "                \"enable_xformers_memory_efficient_attention\": config_enable_xformers_memory_efficient_attention.value\n",
        "            }\n",
        "\n",
        "            # Specify the folder path you want to check\n",
        "            folder_path = Path(f'{INFERENCE_OUTPUT_DIR_PATH}/{config_output_dir_name.value}')\n",
        "\n",
        "            # Check if the folder exists\n",
        "            if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
        "              print(\"\\r\", f'There is already a folder with the name {config_output_dir_name.value}! Please rename your folder!', end=\"\")\n",
        "\n",
        "            else:\n",
        "              # Save updated config back into the yaml file\n",
        "              with open(sample_yaml_path, \"w\") as file:\n",
        "                yaml.dump(config, file, default_style='\"', default_flow_style=False, sort_keys=False)\n",
        "\n",
        "              print(\"\\r\", \"Saving...\", end=\"\")\n",
        "              time.sleep(2)\n",
        "              print(\"\\r\", \"Successfully saved!\", end=\"\")\n",
        "\n",
        "          ## Button Widget and Attach Listener\n",
        "          config_save_btn = widgets.Button(description=\"Save\", layout=configs_config_button_layout)\n",
        "          config_save_btn.on_click(save_config)\n",
        "\n",
        "          # Display fields (same as before)\n",
        "          display(\n",
        "            config_subheader1,\n",
        "            config_vbox,\n",
        "            config_subheader2,\n",
        "            config_vbox_validation_data,\n",
        "            config_save_btn\n",
        "          )\n",
        "\n",
        "          # Update the displayed_widgets list\n",
        "          displayed_widgets.extend([\n",
        "              config_subheader1,\n",
        "              config_vbox,\n",
        "              config_subheader2,\n",
        "              config_vbox_validation_data,\n",
        "              config_save_btn\n",
        "          ])\n",
        "\n",
        "        else:\n",
        "          print(\"The configuration for this yaml is not structured correctly for inference!\")\n",
        "\n",
        "# Attach the event handler to the dropdown's 'value' trait\n",
        "config_files_dropdown.observe(update_load_config, names='value')\n",
        "\n",
        "# Display the dropdown widget and the output widget\n",
        "display(config_files_dropdown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAFxAiNDNHfs"
      },
      "outputs": [],
      "source": [
        "# @title Run FYP Inference { display-mode: \"form\" }\n",
        "\n",
        "# Create layout\n",
        "layout_single_long_button = widgets.Layout(margin='0px 0px 20px 154px')\n",
        "\n",
        "# Create widgets\n",
        "selected_fyp_video_input = widgets.Text(placeholder='Select a video above', description=\"Video:\", disabled=True)\n",
        "config_file_path = widgets.Text(value=Path(f\"{CONFIG_DIR_PATH}/pose_sample.yaml\").as_posix(),description=\"Config File:\")\n",
        "inf_fyp_button = widgets.Button(description=\"Start Inference\", disabled=True, layout=layout_single_long_button)\n",
        "\n",
        "# Create listeners\n",
        "## Retrieve video input\n",
        "def update_fyp_input_video(change):\n",
        "    selected_video = video_dropdown.value\n",
        "    selected_fyp_video_input.value = Path(f\"{VIDEO_DIR_PATH}/{video_subdir_dropdown.value}/{selected_video}\").as_posix()\n",
        "    if selected_video:\n",
        "        inf_fyp_button.disabled = False\n",
        "    else:\n",
        "        inf_fyp_button.disabled = True\n",
        "\n",
        "## Run FYP\n",
        "def run_fyp_inference(button):\n",
        "    %cd -q {FYP_DIR_PATH}\n",
        "\n",
        "    config_file_path_text = config_file_path.value\n",
        "    video_file_path_text = selected_fyp_video_input.value\n",
        "\n",
        "    # Specify the folder path you want to check\n",
        "    with open(config_file_path_text, 'r') as yaml_file:\n",
        "      load_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
        "    folder_path = load_config['output_dir']\n",
        "\n",
        "    # Check if the folder exists\n",
        "    if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
        "      print(\"\\r\", f\"The folder path {load_config['output_dir']} already exists! Please rename your folder!\", end=\"\")\n",
        "\n",
        "    else:\n",
        "      print(\"\\r\", \"\", end=\"\")\n",
        "      # Start inference\n",
        "      # For cross-platform compatibility, we do not run with flag TORCH_DISTRIBUTED_DEBUG=DETAIL\n",
        "      !accelerate launch txt2video.py \\\n",
        "          --config={config_file_path_text}  \\\n",
        "          --skeleton_path={video_file_path_text}\n",
        "\n",
        "## Pass on skeleton path\n",
        "def get_skeleton_path():\n",
        "    skeleton_path = Path(selected_fyp_video_input.value)\n",
        "    return skeleton_path\n",
        "\n",
        "## Pass on config path\n",
        "def get_config_path():\n",
        "    config_path = Path(config_file_path.value)\n",
        "    return config_path\n",
        "\n",
        "# Attach Listeners\n",
        "inf_fyp_button.on_click(run_fyp_inference)\n",
        "video_fyp_button.on_click(update_fyp_input_video)\n",
        "\n",
        "# Display fields\n",
        "display(selected_fyp_video_input)\n",
        "display(config_file_path)\n",
        "display(inf_fyp_button)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EmXta0m2ioal"
      },
      "outputs": [],
      "source": [
        "#@title Post-Process Inference Functions\n",
        "\n",
        "# View the duration, number of frames and fps of a given gif\n",
        "def gif_stats(gif_path):\n",
        "  gif = imageio.get_reader(gif_path)\n",
        "\n",
        "  # Get the duration in seconds\n",
        "  duration = 0.0\n",
        "  for frame in gif:\n",
        "      duration += frame.meta['duration'] / 1000.0\n",
        "\n",
        "  fps = len(gif) / duration\n",
        "  print(f\"Duration: {duration}s, Frames: {len(gif)}, FPS: {fps}\")\n",
        "\n",
        "# Combines all gifs with parameters to show superimposition and captions\n",
        "def postprocess_gif(inf_path, pose_type, display_superimposed, display_captions, is_combine):\n",
        "\n",
        "  clips = []\n",
        "  raw_gifs = []\n",
        "\n",
        "  for gif in os.listdir(Path(f\"{inf_path}/raw\")):\n",
        "    if gif.endswith(\".gif\"):\n",
        "      raw_gifs.append((gif, gif[:-4]))\n",
        "\n",
        "  reader = imageio.get_reader(Path(f\"{inf_path}/raw/{raw_gifs[0][0]}\"))\n",
        "  width, height, _ = reader.get_data(0).shape\n",
        "\n",
        "  # Get pose type\n",
        "  pose_type_dict = {\n",
        "    \"Pose\": (\"pose.gif\", \"P\"),\n",
        "    \"Human\": (\"pose-human.gif\", \"H\"),\n",
        "    \"Human + Pose\": (\"pose-bg.gif\", \"HP\"),\n",
        "  }\n",
        "  pose_path = Path(f\"{inf_path}/{pose_type_dict[pose_type][0]}\")\n",
        "  pose_video = VideoFileClip(pose_path.as_posix()).resize((width,height))\n",
        "\n",
        "  black_clip = ColorClip(size=(pose_video.w, pose_video.h + 40), color=(0, 0, 0), duration=pose_video.duration)\n",
        "\n",
        "  if display_captions:\n",
        "    clips.append(CompositeVideoClip([black_clip, pose_video]))\n",
        "  else:\n",
        "    clips.append(pose_video)\n",
        "\n",
        "  # Get clips\n",
        "  video_path_template = \"{}/superimposed/{}.gif\" if display_superimposed else \"{}/raw/{}.gif\"\n",
        "  for _, prompt in raw_gifs:\n",
        "    file_path = Path(video_path_template.format(inf_path, prompt))\n",
        "    gif_video = VideoFileClip(file_path.as_posix())\n",
        "    if display_captions:\n",
        "      txt_clip = TextClip(prompt, font=\"Amiri-bold\", fontsize=30, color='white')\n",
        "      txt_clip = txt_clip.set_duration(gif_video.duration)\n",
        "      txt_clip = txt_clip.set_position((\"center\", \"bottom\"))\n",
        "      components = [black_clip, txt_clip, gif_video] if is_combine else [black_clip, gif_video]\n",
        "      clips.append(CompositeVideoClip(components))\n",
        "    else:\n",
        "      clips.append(gif_video)\n",
        "\n",
        "  suffix = f\"{pose_type_dict[pose_type][1]}_{'S' if display_superimposed else 'X'}_{'C' if display_captions else 'X'}_{'C' if is_combine else 'X'}\"\n",
        "\n",
        "  if is_combine:\n",
        "    gif_output_path = Path(f\"{inf_path}/processed/all_combined_{suffix}.gif\")\n",
        "    result = clips_array([clips])\n",
        "    result.write_gif(gif_output_path, fps=10, verbose=False, logger=None)\n",
        "  else:\n",
        "    for i, (path, prompt) in enumerate(raw_gifs):\n",
        "      gif_output_path = Path(f\"{inf_path}/processed/{prompt}_{suffix}.gif\")\n",
        "      result = clips_array([[clips[0], clips[i+1]]])\n",
        "      result.write_gif(gif_output_path, fps=10, verbose=False, logger=None)\n",
        "\n",
        "# Matches the skeleton video duration and fps to be the same as gif\n",
        "def postprocess_mmpose(skeleton_path, video_length, output_dir):\n",
        "\n",
        "  # Get the video duration using ffprobe\n",
        "  duration = float(subprocess.check_output(['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1', skeleton_path]))\n",
        "\n",
        "  # Set your desired duration\n",
        "  # video_length is defined original from the sample yaml\n",
        "  # Each unit of video length has been calculated to be 130 ms\n",
        "  desired_duration = video_length * 130 / 1000\n",
        "\n",
        "  # Calculate the speedup factor\n",
        "  speedup_factor = desired_duration / duration\n",
        "\n",
        "  fps = video_length / desired_duration\n",
        "\n",
        "  original_path, bg_path = read_video_metadata(skeleton_path)\n",
        "  print(original_path, bg_path)\n",
        "\n",
        "  # Run the FFmpeg command to adjust the video's duration\n",
        "  output_pose_path = Path(f'{output_dir}/pose.gif')\n",
        "  output_pose_bg_path = Path(f'{output_dir}/pose-bg.gif')\n",
        "  output_pose_human_path = Path(f'{output_dir}/pose-human.gif')\n",
        "\n",
        "  !ffmpeg -i $skeleton_path -vf \"setpts=$speedup_factor*PTS,fps=$fps\" $output_pose_path\n",
        "  !ffmpeg -i $bg_path -vf \"setpts=$speedup_factor*PTS,fps=$fps\" $output_pose_bg_path\n",
        "  !ffmpeg -i $original_path -vf \"setpts=$speedup_factor*PTS,fps=$fps\" $output_pose_human_path\n",
        "\n",
        "# Superimposes all gifs with pose in selected folder\n",
        "def superimposeSkeleton(inf_folder_path):\n",
        "  raw_path = Path(f\"{inf_folder_path}/raw\")\n",
        "  skeleton = Path(f\"{inf_folder_path}/pose.gif\")\n",
        "\n",
        "  output_folder_path = Path(f\"{inf_folder_path}/superimposed\")\n",
        "\n",
        "  # Check if the superimposed folder exists\n",
        "  if not os.path.exists(output_folder_path):\n",
        "    # If it doesn't exist, create the superimposed folder\n",
        "    os.makedirs(output_folder_path)\n",
        "\n",
        "  # Get a list of all files in the directory\n",
        "  generated_gifs = [f for f in os.listdir(raw_path) if os.path.isfile(os.path.join(raw_path, f))]\n",
        "\n",
        "  for human_gif in generated_gifs:\n",
        "    if not human_gif.endswith(\".gif\"):\n",
        "      continue\n",
        "\n",
        "    human_path = Path(f\"{raw_path}/{human_gif}\")\n",
        "    output_file_path = Path(f\"{output_folder_path}/{human_gif}\")\n",
        "\n",
        "    # Load your two GIFs\n",
        "    fg = imageio.get_reader(skeleton)\n",
        "    bg = imageio.get_reader(human_path)\n",
        "\n",
        "    # Create a writer to save the result as a GIF\n",
        "    output_gif = imageio.get_writer(output_file_path, fps=10, loop=0)  # Adjust the desired frame rate\n",
        "\n",
        "    for i in range(min(len(fg), len(bg))):  # Process frames until one of the GIFs ends\n",
        "        foreground = fg.get_data(i)\n",
        "        background = bg.get_data(i)\n",
        "\n",
        "        foreground = cv2.resize(foreground, (512, 512))\n",
        "        background = cv2.resize(background, (512, 512))\n",
        "\n",
        "        # Creating the alpha mask from the foreground image (e.g., removing the black background)\n",
        "        gray = cv2.cvtColor(foreground, cv2.COLOR_BGR2GRAY)\n",
        "        foreground = foreground.astype(float)\n",
        "        background = background.astype(float)\n",
        "\n",
        "        # Dark pixels filter (0 to 255)\n",
        "        black_mask = (gray <= 50)\n",
        "\n",
        "        # Combine the images based on the mask\n",
        "        outImage = np.where(black_mask[:, :, np.newaxis], background, foreground)\n",
        "\n",
        "        # Convert the frame to uint8\n",
        "        ims = outImage.astype(np.uint8)\n",
        "\n",
        "        # Add the frame to the output GIF\n",
        "        output_gif.append_data(ims)\n",
        "\n",
        "    output_gif.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kjL8F2uoBtRp"
      },
      "outputs": [],
      "source": [
        "#@title Post-Inference\n",
        "\n",
        "# Load the inferred config file\n",
        "with open(get_config_path(), 'r') as yaml_file:\n",
        "  load_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
        "\n",
        "# Retrieve info needed from the config file\n",
        "video_length = load_config['validation_data']['video_length']\n",
        "output_dir = load_config['output_dir']\n",
        "\n",
        "# Proper getting skeleton\n",
        "skeleton_path = get_skeleton_path()\n",
        "\n",
        "# Create a pose gif of all 3 types of pose (Human, Skeleton, Superimposed)\n",
        "postprocess_mmpose(skeleton_path, video_length, output_dir)\n",
        "\n",
        "# Superimposes the pose onto generated gifs\n",
        "superimposeSkeleton(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px07rvx7MLoh"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/keypoints.zip /content/inference_output/keypoints_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QY3Tji0jyDR"
      },
      "source": [
        "#### **Combine Gifs**\n",
        "\n",
        "**Inference Directory Name**: Shows a dropdown of folders in the inference output folder that the user can select.\n",
        "\n",
        "**Pose Type**: Shows 3 options as dropdown for user to select\n",
        "> **pose**: Skeleton pose video generated by MMPose with black background <br>\n",
        "> **human**: Original video <br>\n",
        "> **combination**: Skeleton pose superimposed onto the original video\n",
        "\n",
        "**Superimpose on Gif**: Boolean dropdown for user to indicate if they would like to see only the inferred gifs generated or the inferred gifs with the skeleton pose superimposed onto them.\n",
        "\n",
        "**Show Captions**: Boolean dropdown for user to indicate if they would like the prompt to be indicated for the inferred gif generated.\n",
        "\n",
        "**Combine all**: Boolean dropdown for user to indicate if they would like all the inferred gifs generated + the pose gif to be merged into 1 gif for display or separate it into individual inferred gif + skeleton pose gif."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c3VcMWbrS6fp"
      },
      "outputs": [],
      "source": [
        "#@title Combine Gifs\n",
        "\n",
        "def getInferenceRunFolderContent(folder_name):\n",
        "  runfolders = []\n",
        "  for content in os.listdir(INFERENCE_OUTPUT_DIR_PATH):\n",
        "        content_path = os.path.join(INFERENCE_OUTPUT_DIR_PATH, content)\n",
        "\n",
        "        # Check if it's a directory and not hidden\n",
        "        if os.path.isdir(content_path) and not content.startswith(\".\"):\n",
        "            runfolders.append(content)\n",
        "\n",
        "  return runfolders\n",
        "\n",
        "inference_run_folders = os.listdir(INFERENCE_OUTPUT_DIR_PATH)\n",
        "inference_run_folder_content = getInferenceRunFolderContent(inference_run_folders)\n",
        "\n",
        "# Configure layouts\n",
        "combine_gif_button_layout = widgets.Layout(margin='0px 0px 20px 210px', width=\"143px\")\n",
        "combine_gif_style = {'description_width': '200px'}\n",
        "combine_gif_layout = widgets.Layout(width=\"350px\")\n",
        "\n",
        "# Create widgets\n",
        "gif_inference_folder_name = Dropdown(options=inference_run_folder_content, description=\"Inference Directory Name:\", style=combine_gif_style, layout=combine_gif_layout)\n",
        "gif_pose_type = widgets.Dropdown(options=[\"Pose\", \"Human\", \"Human + Pose\"], description='Pose Type:', value=None, style=combine_gif_style, layout=combine_gif_layout)\n",
        "gif_inferred_type = widgets.Dropdown(options=boolean_dropdown, description='Superimpose on Gif:', value=None, style=combine_gif_style, layout=combine_gif_layout)\n",
        "gif_show_captions = widgets.Dropdown(options=boolean_dropdown, description='Show Captions:', value=None, style=combine_gif_style, layout=combine_gif_layout)\n",
        "gif_combine_all = widgets.Dropdown(options=boolean_dropdown, description='Combine all:', value=None, style=combine_gif_style, layout=combine_gif_layout)\n",
        "gif_create_button = widgets.Button(description=\"Create\", disabled=True, layout=combine_gif_button_layout)\n",
        "\n",
        "def clearTextOutput():\n",
        "  clear_output()\n",
        "\n",
        "  display(gif_inference_folder_name)\n",
        "  display(gif_pose_type)\n",
        "  display(gif_inferred_type)\n",
        "  display(gif_show_captions)\n",
        "  display(gif_combine_all)\n",
        "  display(gif_create_button)\n",
        "\n",
        "# Create listeners\n",
        "## Enable button when all dropdowns are populated\n",
        "def enable_button(change):\n",
        "    if gif_pose_type.value and gif_inferred_type.value is not None and gif_show_captions.value is not None and gif_combine_all.value is not None:\n",
        "      gif_create_button.disabled = False\n",
        "    else:\n",
        "      gif_create_button.disabled = True\n",
        "\n",
        "## Display the selected gif\n",
        "def combine_gifs(change):\n",
        "  gif_inf_folder = Path(f\"{INFERENCE_OUTPUT_DIR_PATH}/{gif_inference_folder_name.value}\")\n",
        "  pose_type = gif_pose_type.value\n",
        "  is_superimposed = gif_inferred_type.value\n",
        "  show_captions = gif_show_captions.value\n",
        "  combine_all = gif_combine_all.value\n",
        "\n",
        "  # Check if the folder exists\n",
        "  if os.path.exists(gif_inf_folder) and os.path.isdir(gif_inf_folder):\n",
        "    clearTextOutput()\n",
        "\n",
        "    postprocess_gif(gif_inf_folder, pose_type, is_superimposed, show_captions, combine_all)\n",
        "    print(\"Done\")\n",
        "  else:\n",
        "    print(\"\\r\", 'No such folder exists!', end=\"\")\n",
        "\n",
        "# Attach Listeners\n",
        "gif_pose_type.observe(enable_button, names='value')\n",
        "gif_inferred_type.observe(enable_button, names='value')\n",
        "gif_show_captions.observe(enable_button, names='value')\n",
        "gif_combine_all.observe(enable_button, names='value')\n",
        "gif_create_button.on_click(combine_gifs)\n",
        "\n",
        "# Display fields\n",
        "display(gif_inference_folder_name)\n",
        "display(gif_pose_type)\n",
        "display(gif_inferred_type)\n",
        "display(gif_show_captions)\n",
        "display(gif_combine_all)\n",
        "display(gif_create_button)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RQvzyi1dZmFb"
      },
      "outputs": [],
      "source": [
        "# @title Gif Display\n",
        "def getInferenceRunFolderContent(folder_name):\n",
        "  runfolders = []\n",
        "  for content in os.listdir(INFERENCE_OUTPUT_DIR_PATH):\n",
        "        content_path = os.path.join(INFERENCE_OUTPUT_DIR_PATH, content)\n",
        "\n",
        "        # Check if it's a directory and not hidden\n",
        "        if os.path.isdir(content_path) and not content.startswith(\".\"):\n",
        "            runfolders.append(content)\n",
        "\n",
        "  return runfolders\n",
        "\n",
        "# Init\n",
        "inference_run_folders = os.listdir(INFERENCE_OUTPUT_DIR_PATH)\n",
        "inference_run_folder_content = getInferenceRunFolderContent(inference_run_folders)\n",
        "\n",
        "# Create widgets\n",
        "gif_output_placeholder = widgets.Output(layout=layout_output)\n",
        "gif_subdir_dropdown = widgets.Dropdown(options=inference_run_folder_content, description='Folder:', value=None, disabled=False)\n",
        "gif_dropdown = widgets.Dropdown(options=[], description='Gif:', disabled=True)\n",
        "gif_display_button = widgets.Button(description=\"Display\", disabled=True, layout=layout_double_button)\n",
        "gif_refresh_button = widgets.Button(description=\"Refresh\", disabled=False, layout=layout_double_button)\n",
        "\n",
        "gif_hbox_1 = widgets.HBox([gif_display_button, gif_refresh_button], layout=layout_hbox)\n",
        "\n",
        "# Create listeners\n",
        "## Update gif dropdown options based on the selected folder\n",
        "def gif_subdir_select(change):\n",
        "    selected_gif_folder = gif_subdir_dropdown.value\n",
        "\n",
        "    if selected_gif_folder != None:\n",
        "      selected_GIF_DIR_PATH = Path(f\"{INFERENCE_OUTPUT_DIR_PATH}/{selected_gif_folder}/processed\")\n",
        "      selected_gif_dir_content = [file for file in os.listdir(selected_GIF_DIR_PATH) if file.endswith('.gif')]\n",
        "    else:\n",
        "      selected_gif_dir_content = []\n",
        "\n",
        "    gif_dropdown.options = selected_gif_dir_content\n",
        "    if not selected_gif_dir_content:\n",
        "        gif_dropdown.disabled = False\n",
        "        gif_dropdown.value = None\n",
        "    else:\n",
        "        gif_dropdown.disabled = False\n",
        "\n",
        "## Display the selected gif\n",
        "def display_selected_gif(change):\n",
        "    selected_gif = gif_dropdown.value\n",
        "    selected_gif_folder = gif_subdir_dropdown.value\n",
        "\n",
        "    if selected_gif:\n",
        "        gif_path = Path(f\"{INFERENCE_OUTPUT_DIR_PATH}/{selected_gif_folder}/processed/{selected_gif}\")\n",
        "        gif_display = Image(filename=gif_path, embed=True)\n",
        "\n",
        "        # Clear the output placeholder and display the gif\n",
        "        with gif_output_placeholder:\n",
        "            clear_output()\n",
        "            display(gif_display)\n",
        "\n",
        "## Refresh folder and directory\n",
        "def refresh_folder_and_directory(change):\n",
        "    inference_run_folders = os.listdir(INFERENCE_OUTPUT_DIR_PATH)\n",
        "    inference_run_folder_content = getInferenceRunFolderContent(inference_run_folders)\n",
        "\n",
        "    gif_dropdown.options = []\n",
        "    gif_dropdown.value = None\n",
        "\n",
        "    gif_subdir_dropdown.options = inference_run_folder_content\n",
        "    gif_subdir_dropdown.value = None\n",
        "\n",
        "\n",
        "## Enable button when a valid gif is picked\n",
        "def enable_button(change):\n",
        "    if gif_dropdown.value:\n",
        "        gif_display_button.disabled = False\n",
        "    else:\n",
        "        gif_display_button.disabled = True\n",
        "\n",
        "# Attach Listeners\n",
        "gif_subdir_dropdown.observe(gif_subdir_select, 'value')\n",
        "gif_display_button.on_click(display_selected_gif)\n",
        "gif_refresh_button.on_click(refresh_folder_and_directory)\n",
        "gif_dropdown.observe(enable_button, 'value')\n",
        "\n",
        "\n",
        "# Display fields\n",
        "display(gif_output_placeholder)\n",
        "display(gif_subdir_dropdown)\n",
        "display(gif_dropdown)\n",
        "\n",
        "display(gif_hbox_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFRHv5DxoaJQ"
      },
      "source": [
        "## Training ðŸ‹\n",
        "\n",
        "1. Play the Init cell.\n",
        "2. Play the Dataset Preload (Video) cell.\n",
        "3. Select a dataset folder and click **Start Cutting**.\n",
        "4. Play Dataset Preload (Metadata) cell.\n",
        "5. Play the Training Configuration cell.\n",
        "6. Update the configurations and click **Save**. (Details for each field will be shown above the cell).\n",
        "7. Play Run Training cell.\n",
        "8. Done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Dx3MheZUWM0"
      },
      "outputs": [],
      "source": [
        "# @title Init { display-mode: \"form\" }\n",
        "\n",
        "# Charades Data Class from csv\n",
        "class CharadesData:\n",
        "  def __init__(self, row):\n",
        "    id, subject, scene, quality, relevance, verified, script, objects, descriptions, actions, length = row\n",
        "    self.id = id\n",
        "    self.subject = subject\n",
        "    self.scene = scene\n",
        "    self.quality = quality\n",
        "    self.relevance = relevance\n",
        "    self.verified = verified\n",
        "    self.script = script\n",
        "    self.objects = objects.split(\";\")\n",
        "    self.descriptions = descriptions\n",
        "    self.length = length\n",
        "    self.actions = {}\n",
        "\n",
        "    # Convert actions in proper data structure (\"class_id time_start time_end\" -> class_id: [time_start, time_end])\n",
        "    if len(actions) != 0:\n",
        "      action_substrings = actions.split(';')\n",
        "      for substring in action_substrings:\n",
        "        parts = substring.split()\n",
        "        key = parts[0]\n",
        "        values = [self.convert_to_ms(parts[1]), self.convert_to_ms(parts[2])]\n",
        "        self.actions[key] = values\n",
        "\n",
        "  # For printing\n",
        "  def __str__(self):\n",
        "        return f\"ID: {self.id}, Subject: {self.subject}, Scene: {self.scene}, Quality: {self.quality}, Relevance: {self.relevance}, Verified: {self.verified}, Script: {self.script}, Objects: {self.objects}, Descriptions: {self.descriptions}, Actions: {self.actions}, Length: {self.length}\"\n",
        "\n",
        "  # Helper function to convert time into ms\n",
        "  def convert_to_ms(self, seconds):\n",
        "    ss,ms = seconds.split('.')\n",
        "    total_ms = 1000*int(ss) + int(ms)\n",
        "    return total_ms\n",
        "\n",
        "  # Caption getter with template\n",
        "  def getCaption(self, index):\n",
        "    return f\"In a {self.scene} setting, within the context of '{self.script}', the action '{action_descriptions[list(self.actions.keys())[index]]}' is taking place.\"\n",
        "\n",
        "\n",
        "action_descriptions = {}\n",
        "charades_all = []\n",
        "\n",
        "# Load classes lookup table\n",
        "with open(Path(f\"{CHARADES_LOOKUP_PATH}/Charades_v1_classes.txt\"), 'r') as file:\n",
        "    for line in file:\n",
        "        code, description = line.strip().split(' ', 1)\n",
        "        action_descriptions[code] = description\n",
        "\n",
        "# Load charades data A\n",
        "with open(Path(f\"{CHARADES_LOOKUP_PATH}/Charades_v1_train.csv\"), mode='r') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    next(csv_reader, None)\n",
        "\n",
        "    for row in csv_reader:\n",
        "        charadeData = CharadesData(row)\n",
        "        charades_all.append(charadeData)\n",
        "\n",
        "# Load charades data B\n",
        "with open(Path(f\"{CHARADES_LOOKUP_PATH}/Charades_v1_test.csv\"), mode='r') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    next(csv_reader, None)\n",
        "\n",
        "    for row in csv_reader:\n",
        "        charadeData = CharadesData(row)\n",
        "        charades_all.append(charadeData)\n",
        "\n",
        "clear_output()\n",
        "print(\"Data load successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUJI5iBUf7Ym"
      },
      "outputs": [],
      "source": [
        "# @title Dataset Preload (Video) { display-mode: \"form\" }\n",
        "\n",
        "# Init\n",
        "dataset_dir_folders = os.listdir(VIDEO_DIR_PATH)\n",
        "\n",
        "## Training env\n",
        "training_dataset = None\n",
        "training_idx = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "training_branch = Path(f\"{TRAINING_CONTENT_DIR_PATH}/{training_idx}\")\n",
        "\n",
        "# Create layout\n",
        "config_style = {'description_width': '100px'}\n",
        "config_layout = widgets.Layout(width=\"300px\")\n",
        "config_button_layout = widgets.Layout(margin='0px 0px 20px 154px')\n",
        "\n",
        "# Create widgets\n",
        "dataset_folder_dropdown = widgets.Dropdown(options=[dir for dir in dataset_dir_folders if not dir.startswith(\".\")], description='Dataset Folder:', value=None, layout=config_layout, style=config_style)\n",
        "dataset_cutting_button = widgets.Button(description=\"Start Cutting\", disabled=True, layout=config_button_layout)\n",
        "\n",
        "# Create listeners\n",
        "def dataset_dir_select(change):\n",
        "  if dataset_folder_dropdown:\n",
        "    dataset_folder_content = os.listdir(Path(f\"{VIDEO_DIR_PATH}/{dataset_folder_dropdown.value}\"))\n",
        "    total_dataset_videos = len([file for file in dataset_folder_content if file.endswith('.mp4')])\n",
        "\n",
        "    dataset_cutting_button.disabled = False\n",
        "    dataset_cutting_button.description = f\"Start Cutting ({total_dataset_videos})\"\n",
        "\n",
        "def video_cutting_select(change):\n",
        "  os.makedirs(training_branch, exist_ok=True)\n",
        "  training_dataset = Path(f\"{VIDEO_DIR_PATH}/{dataset_folder_dropdown.value}\")\n",
        "\n",
        "  # Loop video files from selected dataset folder\n",
        "  for video_file in os.listdir(training_dataset):\n",
        "    video, ext = os.path.splitext(video_file)\n",
        "\n",
        "    # Ignore non video files (Eg: .ipynb_checkpoint and csv)\n",
        "    if ext != \".mp4\":\n",
        "      continue\n",
        "\n",
        "    video_folder = Path(f\"{training_branch}/{video}\")\n",
        "    if not os.path.exists(video_folder):\n",
        "      os.mkdir(video_folder)\n",
        "\n",
        "      # Retrieve charade object by ID\n",
        "      charade_data = None\n",
        "      for charade in charades_all:\n",
        "        if charade.id == video:\n",
        "            charade_data = charade\n",
        "            break\n",
        "\n",
        "      # If no clipping required, keep whole video\n",
        "      if not charade_data.actions:\n",
        "        print(f\"No clipping needed\")\n",
        "      else:\n",
        "        print(f\"Clipping {video}\")\n",
        "        charade_actions = charade_data.actions.items()\n",
        "        total_charade_actions = len(charade_actions)\n",
        "        for i, (class_id, timings) in enumerate(charade_actions):\n",
        "\n",
        "          input_video = Path(f\"{training_dataset}/{video_file}\")\n",
        "          output_video = Path(f\"{video_folder}/{video}{i+1:02}{ext}\")\n",
        "\n",
        "          print(f\"#{i+1}/{total_charade_actions}: {timings[0]}ms to {timings[0]+timings[1]}ms [I:{input_video}] [O:{output_video}]\")\n",
        "          !ffmpeg -i {input_video} -ss {timings[0]}ms -t {timings[1]}ms -c:v libx264 -c:a aac {output_video} -loglevel quiet\n",
        "    else:\n",
        "      print(f\"Folder already exist for video_id: {video}. Skipping ...\")\n",
        "\n",
        "  print(\"Finished Clipping\")\n",
        "\n",
        "# Attach Listeners\n",
        "dataset_folder_dropdown.observe(dataset_dir_select, 'value')\n",
        "dataset_cutting_button.on_click(video_cutting_select)\n",
        "\n",
        "# Display fields\n",
        "display(dataset_folder_dropdown)\n",
        "display(dataset_cutting_button)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dff8K4IXmuOm"
      },
      "outputs": [],
      "source": [
        "# @title Dataset Preload (Metadata) { display-mode: \"form\" }\n",
        "\n",
        "training_metadata_file = Path(f\"{training_branch}/metadata.tsv\")\n",
        "with open(training_metadata_file, 'w', newline='', encoding='utf-8') as tsvfile:\n",
        "  fieldnames = ['part_id', 'clip_id', 'caption']\n",
        "  writer = csv.DictWriter(tsvfile, fieldnames=fieldnames, delimiter='\\t')\n",
        "  writer.writeheader()\n",
        "\n",
        "  for part_id in os.listdir(training_branch):\n",
        "    folder_path = os.path.join(training_branch, part_id)\n",
        "\n",
        "    # Ignore non video files (Eg: .ipynb_checkpoint and csv)\n",
        "    if not os.path.isdir(folder_path) or part_id.startswith(\".\"):\n",
        "      continue\n",
        "\n",
        "    charade_data = None\n",
        "    for charade in charades_all:\n",
        "      if charade.id == part_id:\n",
        "          charade_data = charade\n",
        "          break\n",
        "\n",
        "    if not charade_data:\n",
        "      print(\"Missing charades data, skipping ...\")\n",
        "      continue\n",
        "\n",
        "    # Sort by video sub-id to maintain order\n",
        "    training_video_files = sorted(os.listdir(folder_path), key=lambda x: int(os.path.splitext(x)[0][-2:]))\n",
        "    for i, clip in enumerate(training_video_files):\n",
        "      caption = charade_data.getCaption(i)\n",
        "      writer.writerow({\n",
        "          'part_id': part_id,\n",
        "          'clip_id': clip,\n",
        "          'caption': caption\n",
        "      })\n",
        "print(f\"TSV created: {training_metadata_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saKOwITokAKg"
      },
      "source": [
        "#### **Training Configuration** âš™ï¸\n",
        "\n",
        "**pretrained_model_path**: The path that contains the model to fine-tune. This will be a dropdown for the user to select from.\n",
        "\n",
        "**output_dir**: The path where the newly fine tuned model is pushed to. The folder name itself is written by the user, whereas the path to the folder is currently fixed.\n",
        "\n",
        "**Train Data**:\n",
        "> **video_path**: The path that contains the training dataset.\n",
        ">\n",
        "> **n_sample_frames**: Determines how many frames are referenced for training.\n",
        ">\n",
        "> **width**: Resolution of the video.\n",
        ">\n",
        "> **sample_frame_rate**: The rate at which the frames are sampled from. If the sample frames are set to 10 and the frame rate is set to 2, then every second 2 frames are referenced.\n",
        "\n",
        "**learning_rate** The rate at which each step of the training is conducted.\n",
        "\n",
        "**train_batch_size**: How much training can be done together at once. (Larger batch means faster training at the cost of higher memory usage)\n",
        "\n",
        "**max_train_steps**: The number of iterations the dataset is ran to optimize training.\n",
        "\n",
        "**trainable_modules**: The modules that are being trained (No change to be made be a user as the training state would be the same unless requirements change)\n",
        "\n",
        "**seed**: A set training seed to limit and control randomness and ensure reproducibility in case of error and/or for debugging.\n",
        "\n",
        "**mixed_precision**: This is to set the type of precision for text encoding and VAE autoencoding weights. By default, this is set to single precision which is fp32. (High precision in exchange for more memory usage and computational resources used)\n",
        "\n",
        "**use_8bit_adam**: Can be toggled true to reduce memory usage and computational resources used by using 8 bit precision for some part of ADAM optimization computations.\n",
        "\n",
        "**gradient_checkpointing**: Reduces memory usage by doing some checkpoints for gradients, which increases the computational load. Decreases memory usage for increased time taken for training completion.\n",
        "\n",
        "**enable_xformers_memory_efficient_attention**: Reduce memory usage in exchange for slight dip in training performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXhTK71O1Rku"
      },
      "outputs": [],
      "source": [
        "# @title Training Configuration { display-mode: \"form\" }\n",
        "\n",
        "# Init\n",
        "%cd -q {FYP_DIR_PATH}\n",
        "\n",
        "# Initialize load_config with a default value\n",
        "load_config = None\n",
        "\n",
        "# Get a list of all files in the directory\n",
        "config_files = [f for f in os.listdir(CONFIG_DIR_PATH) if os.path.isfile(os.path.join(CONFIG_DIR_PATH, f))]\n",
        "\n",
        "# Define a container for displayed widgets\n",
        "displayed_widgets = []\n",
        "\n",
        "# Create a dropdown widget with the list of config files\n",
        "config_files_dropdown = Dropdown(\n",
        "  options=[\"- Select an Item -\"] + config_files,\n",
        "  description='Select a Config File:',\n",
        "  layout=Layout(width=\"500px\"),\n",
        "  style={'description_width': '150px'}\n",
        ")\n",
        "\n",
        "# Function to clear displayed widgets (excluding the dropdown)\n",
        "def clear_displayed_widgets():\n",
        "    for widget in displayed_widgets:\n",
        "        widget.close()\n",
        "    displayed_widgets.clear()\n",
        "    display(config_files_dropdown)  # Display the dropdown again\n",
        "\n",
        "# Function to update the load_config variable based on the selected filename\n",
        "def update_load_config(change):\n",
        "    global load_config\n",
        "    selected_filename = change.new\n",
        "    if selected_filename and selected_filename != \"- Select an Item -\":\n",
        "        clear_output(wait=True)  # Clear the output area\n",
        "        clear_displayed_widgets()  # Clear previously displayed widgets\n",
        "\n",
        "        # Load yaml file\n",
        "        sample_yaml_path = Path(f\"{CONFIG_DIR_PATH}/{selected_filename}\")\n",
        "        # load_config = OmegaConf.load(sample_yaml_path)\n",
        "        with open(sample_yaml_path, 'r') as yaml_file:\n",
        "          load_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
        "        print(f\"Editing config from: {sample_yaml_path}\")\n",
        "\n",
        "        # Check if the yaml configuration matches expected training config\n",
        "        if compare_dict_structure(expected_training_config, load_config):\n",
        "\n",
        "          # Create a list of model names and paths in the directory\n",
        "          model_options = create_model_list()\n",
        "          video_options = create_video_path_list()\n",
        "          video_path_default = Path(load_config[\"train_data\"][\"video_path\"])\n",
        "\n",
        "          if not (os.path.exists(video_path_default) and os.path.isdir(video_path_default)):\n",
        "            video_path_default = TRAINING_CONTENT_DIR_PATH\n",
        "\n",
        "          value_model_path = Path(load_config['pretrained_model_path'])\n",
        "          if value_model_path not in model_options:\n",
        "            value_model_path = model_options[0][1]\n",
        "          # =====================================\n",
        "          ## Basic Data\n",
        "          config_subheader1 = widgets.HTML(value=\"<h3>Basic Data</h3>\")\n",
        "          config_pretrained_model_path = Dropdown(options=model_options, description=\"pretrained_model_path:\", value=value_model_path, style=configs_config_style, layout=configs_config_layout)\n",
        "          config_output_dir_name = widgets.Text(description=\"output_dir_name:\", value=\"\", style=configs_config_style, layout=configs_config_layout)\n",
        "          config_learning_rate = widgets.FloatText(description=\"learning_rate:\", value=load_config[\"learning_rate\"], style=configs_config_style, layout=configs_config_layout)\n",
        "          # =====================================\n",
        "          # train_data\n",
        "          config_subheader2 = widgets.HTML(value=\"<h3>Train Data</h3>\")\n",
        "          config_video_path = Dropdown(options=video_options, description=\"video_path:\", value=video_path_default, style=configs_config_style, layout=configs_config_layout)\n",
        "          config_n_sample_frames = widgets.IntText(description=\"n_sample_frames:\", value=load_config[\"train_data\"][\"n_sample_frames\"], style=configs_config_style, layout=configs_config_layout)\n",
        "          config_train_data_width = widgets.IntText(description=\"width:\", value=load_config[\"train_data\"][\"width\"], style=configs_config_style, layout=configs_config_layout)\n",
        "          config_sample_frame_rate = widgets.IntText(description=\"sample_frame_rate:\", value=load_config[\"train_data\"][\"sample_frame_rate\"], style=configs_config_style, layout=configs_config_layout)\n",
        "          # =====================================\n",
        "          config_train_batch_size = widgets.IntText(description=\"train_batch_size:\", value=load_config[\"train_batch_size\"], style=configs_config_style, layout=configs_config_layout)\n",
        "          config_max_train_steps = widgets.IntText(description=\"max_train_steps:\", value=load_config[\"max_train_steps\"], style=configs_config_style, layout=configs_config_layout)\n",
        "          config_seed = widgets.IntText(description=\"seed:\", value=load_config[\"seed\"], style=configs_config_style, layout=configs_config_layout)\n",
        "          config_mixed_precision = widgets.Text(description=\"mixed_precision:\", value=load_config[\"mixed_precision\"], style=configs_config_style, layout=configs_config_layout)\n",
        "          config_use_8bit_adam = Dropdown(options=boolean_dropdown, value=load_config[\"use_8bit_adam\"], description=\"config_use_8bit_adam:\", style=configs_config_style, layout=configs_config_layout)\n",
        "          config_gradient_checkpointing = Dropdown(options=boolean_dropdown, value=load_config[\"gradient_checkpointing\"], description=\"gradient_checkpointing:\", style=configs_config_style, layout=configs_config_layout)\n",
        "          config_enable_xformers_memory_efficient_attention = Dropdown(options=boolean_dropdown, value=load_config[\"enable_xformers_memory_efficient_attention\"], description=\"enable_xformers_memory_efficient_attention:\", style=configs_config_style, layout=configs_config_layout)\n",
        "          # =====================================\n",
        "          ## Button Widget\n",
        "          config_save_btn = widgets.Button(description=\"Save\", layout=configs_config_button_layout)\n",
        "          ## Group widgets\n",
        "          config_vbox = widgets.VBox([\n",
        "              config_pretrained_model_path,\n",
        "              config_output_dir_name,\n",
        "              config_learning_rate,\n",
        "              config_train_batch_size,\n",
        "              config_max_train_steps,\n",
        "              config_seed,\n",
        "              config_mixed_precision,\n",
        "              config_use_8bit_adam,\n",
        "              config_gradient_checkpointing,\n",
        "              config_enable_xformers_memory_efficient_attention\n",
        "          ])\n",
        "          config_vbox_train_data = widgets.VBox([\n",
        "              config_video_path,\n",
        "              config_n_sample_frames,\n",
        "              config_train_data_width,\n",
        "              config_sample_frame_rate,\n",
        "          ])\n",
        "\n",
        "          # Display fields (same as before)\n",
        "          display(\n",
        "            config_subheader1,\n",
        "            config_vbox,\n",
        "            config_subheader2,\n",
        "            config_vbox_train_data,\n",
        "            config_save_btn\n",
        "          )\n",
        "\n",
        "          # Create listeners\n",
        "          def save_config(change):\n",
        "            config = {\n",
        "                \"pretrained_model_path\": Path(config_pretrained_model_path.value).as_posix(),\n",
        "                \"output_dir\": Path(f\"{CUSTOM_MODEL_DIR_PATH}/{config_output_dir_name.value}\").as_posix(),\n",
        "                \"train_data\": {\n",
        "                    \"video_path\": Path(config_video_path.value).as_posix(),\n",
        "                    \"n_sample_frames\": config_n_sample_frames.value,\n",
        "                    \"width\": config_train_data_width.value,\n",
        "                    \"sample_frame_rate\": config_sample_frame_rate.value\n",
        "                },\n",
        "                \"learning_rate\": config_learning_rate.value,\n",
        "                \"train_batch_size\": config_train_batch_size.value,\n",
        "                \"max_train_steps\": config_max_train_steps.value,\n",
        "                \"trainable_modules\": load_config[\"trainable_modules\"],\n",
        "                \"seed\": config_seed.value,\n",
        "                \"mixed_precision\": config_mixed_precision.value,\n",
        "                \"use_8bit_adam\": config_use_8bit_adam.value,\n",
        "                \"gradient_checkpointing\": config_gradient_checkpointing.value,\n",
        "                \"enable_xformers_memory_efficient_attention\": config_enable_xformers_memory_efficient_attention.value\n",
        "            }\n",
        "\n",
        "            if Path(config_video_path.value) == TRAINING_CONTENT_DIR_PATH:\n",
        "              print(\"\\r\", \"Please choose a training folder!\", end=\"\")\n",
        "\n",
        "            else:\n",
        "              #Save updated config back into yaml file\n",
        "              with open(sample_yaml_path, \"w\") as file:\n",
        "                yaml.dump(config, file, default_style='\"', default_flow_style=False, sort_keys=False)\n",
        "\n",
        "              print(\"\\r\", \"Saving...\", end=\"\")\n",
        "              time.sleep(2)\n",
        "              print(\"\\r\", \"Successfully saved!\", end=\"\")\n",
        "\n",
        "          # Attach Listeners\n",
        "          config_save_btn.on_click(save_config)\n",
        "\n",
        "          # Update the displayed_widgets list\n",
        "          displayed_widgets.extend([\n",
        "              config_subheader1,\n",
        "              config_vbox,\n",
        "              config_subheader2,\n",
        "              config_vbox_train_data,\n",
        "              config_save_btn,\n",
        "          ])\n",
        "\n",
        "        else:\n",
        "          print(\"The configuration for this yaml is not structured correctly for training!\")\n",
        "\n",
        "# Attach the event handler to the dropdown's 'value' trait\n",
        "config_files_dropdown.observe(update_load_config, names='value')\n",
        "\n",
        "# Display the dropdown widgets\n",
        "display(config_files_dropdown)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8Guk7wZlnsU"
      },
      "source": [
        "#### **Perform Training** â–¶ï¸\n",
        "\n",
        "1. Running the code would immediately start the training process.\n",
        "\n",
        "2. Once the training process starts, there will be a 2 minute buffer to load the necessary data for training.\n",
        "\n",
        "3. After the buffer, a progress bar would show displaying the progress of the training together with the percentage of completion.\n",
        "\n",
        "4. Finally when the training is completed, the newly generated model will be saved in the output directory path set by the user in the Training Configuration section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW2DKILlW1Hp"
      },
      "outputs": [],
      "source": [
        "# @title Run Training { display-mode: \"form\" }\n",
        "\n",
        "%cd -q {FYP_DIR_PATH}\n",
        "\n",
        "output_label = widgets.Label(value=\"Output will appear here:\")\n",
        "display(output_label)\n",
        "\n",
        "def run_command_and_display_output(command):\n",
        "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True, shell=True)\n",
        "    for line in process.stdout:\n",
        "        output_label.value = line.strip()  # Update the label with the live output\n",
        "    process.wait()\n",
        "\n",
        "# For cross-platform compatibility, we do not run with flag TORCH_DISTRIBUTED_DEBUG=DETAIL\n",
        "run_command_and_display_output('accelerate launch train_followyourpose.py --config=\"configs/pose_train.yaml\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxyVHKaQIkYs"
      },
      "source": [
        "## Testing (WIP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSTSrg6Ji3gr"
      },
      "source": [
        "Testing Section:\n",
        "\n",
        "\tTesting yaml Configuration:\n",
        "\t\tPrompts and video length only adjustable\n",
        "\tDropdown to select folder to run testing(inference) on\n",
        "\tRun inference through ALL skeletons in folder\n",
        "\tWhen finding original vid OR skeleton bg vid to superimpose\n",
        "\tfind skeleton video via skeleton folder name in testing\n",
        "\tin metadata of skeleton video extract the path of original vid + bg vid\n",
        "\tFor every inferred gif convert to mp4\n",
        "\tAll converted mp4 put through MMPose\n",
        "\tThis generates a keypoint json to be stored in JSON folder under the skeleton name\n",
        "\tRepeat for every skeleton folder created\n",
        "\n",
        "Testing_Output folder <br>\n",
        "  -> model name folder <br>\n",
        "  -> -> Name of each skeleton folder <br>\n",
        "  -> -> -> Inferred Gifs folder, JSON folder <br>\n",
        "  -> -> -> -> X number of inferred gifs, keypoints json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nec8V1kmIuXJ"
      },
      "outputs": [],
      "source": [
        "# @title Test Configuration { display-mode: \"form\" }\n",
        "\n",
        "# Init\n",
        "%cd -q {FYP_DIR_PATH}\n",
        "\n",
        "# Initialize load_config with a default value\n",
        "load_config = None\n",
        "\n",
        "# Define a container for displayed widgets\n",
        "displayed_widgets = []\n",
        "\n",
        "# Get a list of all files in the directory\n",
        "config_files = [f for f in os.listdir(CONFIG_DIR_PATH) if os.path.isfile(os.path.join(CONFIG_DIR_PATH, f))]\n",
        "\n",
        "# Create a dropdown widget with the list of config files\n",
        "config_files_dropdown = Dropdown(\n",
        "  options=[\"- Select an Item -\"] + config_files,\n",
        "  description='Select a Config File:',\n",
        "  layout=Layout(width=\"500px\"),\n",
        "  style={'description_width': '150px'}\n",
        ")\n",
        "\n",
        "# Function to clear displayed widgets (excluding the dropdown)\n",
        "def clear_displayed_widgets():\n",
        "    for widget in displayed_widgets:\n",
        "        widget.close()\n",
        "    displayed_widgets.clear()\n",
        "    display(config_files_dropdown)  # Display the dropdown again\n",
        "\n",
        "# Function to update the load_config variable based on the selected filename\n",
        "def update_load_config(change):\n",
        "    global load_config\n",
        "    selected_filename = change.new\n",
        "    if selected_filename and selected_filename != \"- Select an Item -\":\n",
        "        clear_output(wait=True)  # Clear the output area\n",
        "        clear_displayed_widgets()  # Clear previously displayed widgets\n",
        "\n",
        "        # Load yaml file\n",
        "        sample_yaml_path = f\"{CONFIG_DIR_PATH}/{selected_filename}\"\n",
        "        with open(sample_yaml_path, 'r') as yaml_file:\n",
        "          load_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
        "        print(f\"Editing config from: {sample_yaml_path}\")\n",
        "\n",
        "        # Check if the yaml configuration matches expected training config\n",
        "        if compare_dict_structure(expected_inference_config, load_config):\n",
        "\n",
        "          # Create a list of model names and paths in the directory\n",
        "          model_options = create_model_list()\n",
        "\n",
        "          # =====================================\n",
        "          ## Basic Data\n",
        "          config_subheader1 = widgets.HTML(value=\"<h3>Basic Data</h3>\")\n",
        "          config_pretrained_model_path = Dropdown(options=model_options, description=\"pretrained_model_path:\", value=load_config['pretrained_model_path'], style=configs_config_style, layout=configs_config_layout)\n",
        "          config_output_dir_name = widgets.Text(description=\"output_dir_name:\", value=\"\", style=configs_config_style, layout=configs_config_layout)\n",
        "          # =====================================\n",
        "          ## Validation_data\n",
        "          config_subheader2 = widgets.HTML(value=\"<h3>Validation Data</h3>\")\n",
        "          config_prompts = widgets.Textarea(description=\"prompts:\", value=\"\\n\".join(load_config['validation_data']['prompts']), style=configs_config_style, layout=widgets.Layout(width=\"500px\", height=\"100px\"))\n",
        "          config_video_length = widgets.IntText(description=\"video_length:\", value=load_config['validation_data']['video_length'], style=configs_config_style, layout=configs_config_layout)\n",
        "          config_num_inference_steps = widgets.IntText(description=\"num_inference_steps:\", value=load_config['validation_data']['num_inference_steps'], style=configs_config_style, layout=configs_config_layout)\n",
        "          # =====================================\n",
        "          ## Group widgets\n",
        "          config_vbox = widgets.VBox([\n",
        "              config_pretrained_model_path,\n",
        "              config_output_dir_name\n",
        "          ])\n",
        "          config_vbox_validation_data = widgets.VBox([\n",
        "              config_prompts,\n",
        "              config_video_length,\n",
        "              config_num_inference_steps\n",
        "          ])\n",
        "\n",
        "          # Create listeners\n",
        "          def save_config(change):\n",
        "            config = {\n",
        "                \"pretrained_model_path\": config_pretrained_model_path.value,\n",
        "                \"output_dir\": f\"{TEST_OUTPUT_DIR_PATH}/{config_output_dir_name.value}\",\n",
        "                \"validation_data\": {\n",
        "                    \"prompts\": [prompt.strip() for prompt in config_prompts.value.splitlines() if prompt.strip()],\n",
        "                    \"video_length\": config_video_length.value,\n",
        "                    \"width\": load_config['validation']['width'],\n",
        "                    \"height\": load_config['validation']['height'],\n",
        "                    \"num_inference_steps\": config_num_inference_steps.value,\n",
        "                    \"guidance_scale\": load_config['validation']['guidance_scale'],\n",
        "                    \"use_inv_latent\": load_config['validation']['use_inv_latent'],\n",
        "                    \"num_inv_steps\": load_config['validation']['num_inv_steps'],\n",
        "                    \"dataset_set\": load_config['validation_data']['dataset_set']\n",
        "                },\n",
        "                \"train_batch_size\": load_config['train_batch_size'],\n",
        "                \"validation_steps\": load_config['validation_steps'],\n",
        "                \"resume_from_checkpoint\": load_config['resume_from_checkpoint'],\n",
        "                \"seed\": load_config['seed'],\n",
        "                \"mixed_precision\": load_config['mixed_precision'],\n",
        "                \"gradient_checkpointing\": load_config['gradient_checkpointing'],\n",
        "                \"enable_xformers_memory_efficient_attention\": load_config['enable_xformers_memory_efficient_attention']\n",
        "            }\n",
        "\n",
        "            # Specify the folder path you want to check\n",
        "            folder_path = f'{INFERENCE_OUTPUT_DIR_PATH}/{config_output_dir_name.value}'\n",
        "\n",
        "            # Check if the folder exists\n",
        "            if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
        "              print(\"\\r\", f'There is already a folder with the name {config_output_dir_name.value}! Please rename your folder!', end=\"\")\n",
        "\n",
        "            else:\n",
        "              # Save updated config back into the yaml file\n",
        "              with open(sample_yaml_path, \"w\") as file:\n",
        "                yaml.dump(config, file, default_style='\"', default_flow_style=False, sort_keys=False)\n",
        "\n",
        "              print(\"\\r\", \"Saving...\", end=\"\")\n",
        "              time.sleep(2)\n",
        "              print(\"\\r\", \"Successfully saved!\", end=\"\")\n",
        "\n",
        "          ## Button Widget and Attach Listener\n",
        "          config_save_btn = widgets.Button(description=\"Save\", layout=configs_config_button_layout)\n",
        "          config_save_btn.on_click(save_config)\n",
        "\n",
        "          # Display fields (same as before)\n",
        "          display(\n",
        "            config_subheader1,\n",
        "            config_vbox,\n",
        "            config_subheader2,\n",
        "            config_vbox_validation_data,\n",
        "            config_save_btn\n",
        "          )\n",
        "\n",
        "          # Update the displayed_widgets list\n",
        "          displayed_widgets.extend([\n",
        "              config_subheader1,\n",
        "              config_vbox,\n",
        "              config_subheader2,\n",
        "              config_vbox_validation_data,\n",
        "              config_save_btn\n",
        "          ])\n",
        "\n",
        "        else:\n",
        "          print(\"The configuration for the yaml is not structured correctly for inference\")\n",
        "\n",
        "# Attach the event handler to the dropdown's 'value' trait\n",
        "config_files_dropdown.observe(update_load_config, names='value')\n",
        "\n",
        "# Display the dropdown widget and the output widget\n",
        "display(config_files_dropdown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpAPUQfOl2eM"
      },
      "outputs": [],
      "source": [
        "def getTestRunFolderContent(folder_name):\n",
        "  runfolders = []\n",
        "  for content in os.listdir(TEST_OUTPUT_DIR_PATH):\n",
        "        content_path = os.path.join(TEST_OUTPUT_DIR_PATH, content)\n",
        "\n",
        "        # Check if it's a directory and not hidden\n",
        "        if os.path.isdir(content_path) and not content.startswith(\".\"):\n",
        "            runfolders.append(content)\n",
        "\n",
        "  return runfolders\n",
        "\n",
        "# Init\n",
        "test_run_folders = os.listdir(TEST_OUTPUT_DIR_PATH)\n",
        "test_run_folder_content = getTestRunFolderContent(test_run_folders)\n",
        "\n",
        "test_folder_content_dropdown = widgets.Dropdown(options=test_run_folder_content, description='Folder:', value=None, disabled=False)\n",
        "\n",
        "display(test_folder_content_dropdown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYA6KJlzA2Id"
      },
      "outputs": [],
      "source": [
        "# Load the GIF using VideoFileClip\n",
        "gif_path = f\"/content/inference_output/keypoints_test/pose-human.gif\"\n",
        "gif_clip = VideoFileClip(gif_path)\n",
        "\n",
        "# Define the output MP4 file name\n",
        "mp4_path =  f\"/content/inference_output/keypoints_test/gif_to_video/pose-human.mp4\"\n",
        "\n",
        "os.makedirs(\"/content/inference_output/keypoints_test/gif_to_video\", exist_ok=True)\n",
        "\n",
        "# Write the GIF as an MP4 video\n",
        "gif_clip.write_videofile(mp4_path, codec='libx264')\n",
        "\n",
        "def convertGifs(inf_path):\n",
        "  raw_gifs = [f for f in os.listdir(f\"{inf_raw_path}/raw\") if os.path.isfile(os.path.join(f\"{inf_raw_path}/raw\", f))]\n",
        "  print(raw_gifs)\n",
        "\n",
        "  for gif in raw_gifs:\n",
        "    # Load the GIF using VideoFileClip\n",
        "    gif_path = f\"{inf_path}/raw/{gif}\"\n",
        "    gif_clip = VideoFileClip(gif_path)\n",
        "\n",
        "    os.makedirs(f\"{inf_path}/gif_to_video\", exist_ok=True)\n",
        "\n",
        "    # Define the output MP4 file name\n",
        "    mp4_path =  f\"{inf_path}/gif_to_video/{gif[:-4]}.mp4\"\n",
        "\n",
        "    # Write the GIF as an MP4 video\n",
        "    gif_clip.write_videofile(mp4_path, codec='libx264')\n",
        "\n",
        "inf_raw_path = \"/content/inference_output/keypoints_test\"\n",
        "convertGifs(inf_raw_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t83NrNcfJB36"
      },
      "outputs": [],
      "source": [
        "# @title Run Test { display-mode: \"form\" }\n",
        "\n",
        "# Find the skeleton\n",
        "test_skeletons_path = f\"{VIDEO_DIR_PATH}/Skeleton\"\n",
        "skeletons_folder_content = os.listdir(test_skeletons_path)\n",
        "total_dataset_videos = len([file for file in skeletons_folder_content if file.endswith('.mp4')])\n",
        "\n",
        "# Create layout\n",
        "layout_single_long_button = widgets.Layout(margin='0px 0px 20px 154px')\n",
        "\n",
        "# Create widgets\n",
        "config_file_path = widgets.Text(value='configs/pose_sample.yaml',description=\"Config File:\")\n",
        "test_button = widgets.Button(description=f\"Start Test {total_dataset_videos}\", layout=layout_single_long_button)\n",
        "\n",
        "## Run FYP\n",
        "def run_test(button):\n",
        "    %cd -q {FYP_DIR_PATH}\n",
        "\n",
        "    config_file_path_text = config_file_path.value\n",
        "\n",
        "    # Specify the folder path you want to check\n",
        "    with open(config_file_path_text, 'r') as yaml_file:\n",
        "          load_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
        "    folder_path = load_config['output_dir']\n",
        "\n",
        "    # Check if the folder exists\n",
        "    if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
        "      print(\"\\r\", f'The folder path {load_config[\"output_dir\"]}! Please rename your folder!', end=\"\")\n",
        "\n",
        "    else:\n",
        "      print(\"\\r\", \"\", end=\"\")\n",
        "\n",
        "      for video in skeletons_folder_content:\n",
        "\n",
        "        current_skeleton_path = f\"{test_skeletons_path}/{video}\"\n",
        "        print(\"\\r\", f\"Now starting inference on {video}\")\n",
        "        # Start inference\n",
        "        !TORCH_DISTRIBUTED_DEBUG=DETAIL accelerate launch txt2video.py \\\n",
        "            --config={config_file_path_text}  \\\n",
        "            --skeleton_path={current_skeleton_path}\n",
        "\n",
        "        clear_output()\n",
        "        print(f\"Inference on {video} has been completed!\")\n",
        "\n",
        "# Attach Listeners\n",
        "test_button.on_click(run_test)\n",
        "\n",
        "# Display fields\n",
        "display(config_file_path)\n",
        "display(test_button)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlSJhz4PA8Tw"
      },
      "outputs": [],
      "source": [
        "# @title Comparison MMPose Inference { display-mode: \"form\" }\n",
        "\n",
        "# Helper Functions\n",
        "## Re-encode video due to H.264 video encoding error\n",
        "def reencode_video(input_file):\n",
        "  temp_output_file = f\"{TEST_OUTPUT_DIR_PATH}/output.mp4\"\n",
        "  !ffmpeg -i {input_file} -c:v libx264 -crf 23 -c:a aac -strict experimental {temp_output_file}\n",
        "\n",
        "  os.remove(input_file)\n",
        "  os.rename(temp_output_file, input_file)\n",
        "\n",
        "# Create layout\n",
        "layout_single_long_button = widgets.Layout(margin='0px 0px 20px 154px')\n",
        "\n",
        "# Create widgets\n",
        "selected_mmpose_video_input = widgets.Text(placeholder='Select a video above', description=\"Video:\", disabled=False)\n",
        "inf_mmpose_button = widgets.Button(description=\"Start Inference\", disabled=False, layout=layout_single_long_button)\n",
        "\n",
        "# Create listeners\n",
        "## Retrieve video input\n",
        "def update_mmpose_input_video(change):\n",
        "    selected_video = video_dropdown.value\n",
        "    selected_mmpose_video_input.value = selected_video\n",
        "    if selected_video:\n",
        "        inf_mmpose_button.disabled = False\n",
        "    else:\n",
        "        inf_mmpose_button.disabled = True\n",
        "\n",
        "## Run mmpose\n",
        "def run_mmpose_inference(button):\n",
        "    selected_video = selected_mmpose_video_input.value\n",
        "    video_path = selected_video\n",
        "    videos_to_process = [f for f in os.listdir(video_path) if os.path.isfile(os.path.join(video_path, f))]\n",
        "    folder_name = video_path.split(\"/\")[-2]\n",
        "    print(videos_to_process)\n",
        "\n",
        "    %cd -q {MMPOSE_DIR_PATH}\n",
        "\n",
        "    generated_path = f\"/content/test_output/{folder_name}/generated_poses\"\n",
        "    keypoints_folder = f\"/content/test_output/{folder_name}/keypoints\"\n",
        "\n",
        "    os.makedirs(generated_path, exist_ok=True)\n",
        "    os.makedirs(keypoints_folder, exist_ok=True)\n",
        "\n",
        "    for video in videos_to_process:\n",
        "      processed_video_path = f\"{video_path}/'{video}'\"\n",
        "\n",
        "      # Start inference with black backgroudn\n",
        "      !python demo/inferencer_demo.py \\\n",
        "          {processed_video_path}  \\\n",
        "          --pose2d human \\\n",
        "          --vis-out-dir {generated_path} \\\n",
        "          --black-background \\\n",
        "          --thickness 4 \\\n",
        "          --radius 0 \\\n",
        "          --pred-out-dir {keypoints_folder}\n",
        "\n",
        "      # Change name for human background\n",
        "      input_file = f\"{generated_path}/{video}\"\n",
        "      reencode_video(input_file)\n",
        "\n",
        "      print(\"Done, Outputs:\")\n",
        "      print(f\"Saved at: {input_file}\")\n",
        "\n",
        "    clear_output()\n",
        "    mmpose_inf_display()\n",
        "    print(\"Done\")\n",
        "    print(f\"Saved at: {generated_path}\")\n",
        "\n",
        "# Attach Listeners\n",
        "video_mmpose_button.on_click(update_mmpose_input_video)\n",
        "inf_mmpose_button.on_click(run_mmpose_inference)\n",
        "\n",
        "# Display fields\n",
        "def mmpose_inf_display():\n",
        "  display(selected_mmpose_video_input)\n",
        "  display(inf_mmpose_button)\n",
        "mmpose_inf_display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cCbYH6gA_-X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming skeleton1 and skeleton2 are dictionaries or data structures\n",
        "# with keypoint positions, e.g., skeleton1['keypoints'] and skeleton2['keypoints']\n",
        "with open(\"/content/test_output/testing/keypoints/Astronaut on the beach.json\", 'r') as json_file1:\n",
        "  skeleton1 = json.load(json_file1)\n",
        "\n",
        "with open(\"/content/test_output/testing/keypoints/pose-human.json\", 'r') as json_file2:\n",
        "  skeleton2 = json.load(json_file2)\n",
        "\n",
        "all_keypoints1 = []\n",
        "all_keypoints2 = []\n",
        "\n",
        "for frame in skeleton1:\n",
        "  for instance in frame[\"instances\"]:\n",
        "    keypoints = instance[\"keypoints\"]\n",
        "    all_keypoints1.append(keypoints)\n",
        "\n",
        "for frame in skeleton2:\n",
        "  for instance in frame[\"instances\"]:\n",
        "    keypoints = instance[\"keypoints\"]\n",
        "    all_keypoints2.append(keypoints)\n",
        "\n",
        "# Perform keypoint-based similarity measurement (e.g., using Euclidean distance)\n",
        "def euclidean_distance(p1, p2):\n",
        "    return np.linalg.norm(np.array(p1) - np.array(p2))\n",
        "\n",
        "# Calculate the similarity score based on keypoint positions\n",
        "similarity_scores = []\n",
        "for keypoints1, keypoints2 in zip(all_keypoints1, all_keypoints2):\n",
        "  for point1, point2 in zip(keypoints1, keypoints2):\n",
        "    print(point1)\n",
        "    print(point2)\n",
        "    break\n",
        "    # distance = euclidean_distance(point1, point2)\n",
        "    # similarity_scores.append(distance)\n",
        "\n",
        "# Calculate an overall similarity score (e.g., average or sum of distances)\n",
        "# overall_similarity = sum(similarity_scores) / len(similarity_scores)\n",
        "\n",
        "print(f\"Overall Similarity Score: {overall_similarity}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJM6uN-SYE1M"
      },
      "outputs": [],
      "source": [
        "!unzip /content/keypoints_test.zip -d /content/inference_output"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "HpMOpVG_KhHb",
        "y12b9B4VLmqU",
        "5JFSbnRyryFs"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
