{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome!\n",
    "To ensure outputs are not truncated, press [Shift-O]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### First-Time Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T16:51:13.371716Z",
     "start_time": "2023-10-15T16:33:19.427601Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clone FYP inference base model\n",
    "\n",
    "%cd ..\\FollowYourPose\n",
    "!mkdir checkpoints\n",
    "\n",
    "%cd checkpoints\n",
    "!git lfs install\n",
    "!git clone https://huggingface.co/YueMafighting/FollowYourPose_v1 .\n",
    "%cd ..\\..\\demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsequent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T07:32:46.306699Z",
     "start_time": "2023-10-28T07:32:46.298676Z"
    }
   },
   "outputs": [],
   "source": [
    "# return to root folder\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T07:32:53.417979Z",
     "start_time": "2023-10-28T07:32:51.223247Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Folder Scaffolding & Path Loading\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "'''\n",
    "how 2 Path\n",
    "Try to surround file paths with Path() to reduce confusion\n",
    "Because Path(str / str) does not work, try to use fstrings as inputs to Path(), instead of Path(var / var)\n",
    "This is to reduce the impact of inevitable missed Path()s\n",
    "If need to write filepath as a string, use .as_posix()\n",
    "'''\n",
    "\n",
    "# Root Directory\n",
    "ROOT_DIR_PATH = os.getcwd()\n",
    "\n",
    "# Video\n",
    "VIDEO_DIR_PATH = Path(f\"{ROOT_DIR_PATH}/video\")\n",
    "VIDEO_SKELETON_DIR_PATH = Path(f\"{ROOT_DIR_PATH}/video/Skeleton\")\n",
    "!mkdir {VIDEO_SKELETON_DIR_PATH}\n",
    "\n",
    "# I/O Files\n",
    "TRAINING_CONTENT_DIR_PATH = Path(f\"{ROOT_DIR_PATH}/training_content\")\n",
    "!mkdir {TRAINING_CONTENT_DIR_PATH}\n",
    "CUSTOM_MODEL_DIR_PATH = Path(f\"{ROOT_DIR_PATH}/custom_model\")\n",
    "!mkdir {CUSTOM_MODEL_DIR_PATH}\n",
    "INFERENCE_OUTPUT_DIR_PATH = Path(f\"{ROOT_DIR_PATH}/inference_output\")\n",
    "!mkdir {INFERENCE_OUTPUT_DIR_PATH}\n",
    "TEST_OUTPUT_DIR_PATH = Path(f\"{ROOT_DIR_PATH}/test_output\")\n",
    "!mkdir {TEST_OUTPUT_DIR_PATH}\n",
    "\n",
    "MMPOSE_DIR_PATH = Path(f\"{ROOT_DIR_PATH}/MMPose\")\n",
    "FYP_DIR_PATH = Path(f\"{ROOT_DIR_PATH}/FollowYourPose\")\n",
    "\n",
    "# FYP Config\n",
    "CONFIG_DIR_PATH = Path(f\"{FYP_DIR_PATH}/configs\")\n",
    "\n",
    "# Dataset Paths\n",
    "CHARADES_LOOKUP_PATH = Path(f\"{ROOT_DIR_PATH}/other_files/charades_lookup\")\n",
    "\n",
    "import sys\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import yaml\n",
    "\n",
    "from ipywidgets import Dropdown, Output, Layout, widgets, Button, VBox, HBox\n",
    "from IPython.display import display, Markdown, HTML, Video, Image, clear_output\n",
    "\n",
    "from moviepy.editor import VideoFileClip, clips_array, TextClip, CompositeVideoClip, ColorClip\n",
    "from moviepy.config import change_settings\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imageio\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T07:32:56.091976Z",
     "start_time": "2023-10-28T07:32:56.079849Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Configuration Presets\n",
    "\n",
    "# Expected Configurations of a Inference yaml\n",
    "expected_inference_config = {\n",
    "  \"pretrained_model_path\": str,\n",
    "  \"output_dir\": str,\n",
    "  \"validation_data\": {\n",
    "      \"prompts\": list,\n",
    "      \"video_length\": int,\n",
    "      \"width\": int,\n",
    "      \"height\": int,\n",
    "      \"num_inference_steps\": int,\n",
    "      \"guidance_scale\": float,\n",
    "      \"use_inv_latent\": bool,\n",
    "      \"num_inv_steps\": int,\n",
    "      \"dataset_set\": str\n",
    "  },\n",
    "  \"train_batch_size\": int,\n",
    "  \"validation_steps\": int,\n",
    "  \"resume_from_checkpoint\": str,\n",
    "  \"seed\": int,\n",
    "  \"mixed_precision\": str,\n",
    "  \"gradient_checkpointing\": bool,\n",
    "  \"enable_xformers_memory_efficient_attention\": bool\n",
    "}\n",
    "\n",
    "# Expected Configurations of a Training yaml\n",
    "expected_training_config = {\n",
    "  \"pretrained_model_path\": str,\n",
    "  \"output_dir\": str,\n",
    "  \"train_data\": {\n",
    "    \"video_path\": str,\n",
    "    \"n_sample_frames\": int,\n",
    "    \"width\": int,\n",
    "    \"sample_frame_rate\": int\n",
    "  },\n",
    "  \"learning_rate\": float,\n",
    "  \"train_batch_size\": int,\n",
    "  \"max_train_steps\": int,\n",
    "  \"trainable_modules\": list,\n",
    "  \"seed\": int,\n",
    "  \"mixed_precision\": str,\n",
    "  \"use_8bit_adam\": bool,\n",
    "  \"gradient_checkpointing\": bool,\n",
    "  \"enable_xformers_memory_efficient_attention\": bool\n",
    "}\n",
    "\n",
    "# Define the options for boolean dropdown\n",
    "boolean_dropdown = [True, False]\n",
    "\n",
    "# Create layout\n",
    "configs_config_style = {'description_width': '150px'}\n",
    "configs_config_layout = widgets.Layout(width=\"500px\")\n",
    "configs_config_button_layout = widgets.Layout(margin='0px 0px 20px 354px')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T07:32:58.742917Z",
     "start_time": "2023-10-28T07:32:58.730883Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Configuration Functions\n",
    "\n",
    "# Find all models and creates a list of the models' path in the directory\n",
    "def create_model_list():\n",
    "  model_options = [(\"Default\", Path(f'{FYP_DIR_PATH}/checkpoints/stable-diffusion-v1-4'))]\n",
    "  for model_name in os.listdir(CUSTOM_MODEL_DIR_PATH):\n",
    "    model_path = os.path.join(CUSTOM_MODEL_DIR_PATH, model_name)\n",
    "    if os.path.isdir(model_path):\n",
    "      model_options.append((model_name, Path(f'{CUSTOM_MODEL_DIR_PATH}/{model_name}')))\n",
    "  return model_options\n",
    "\n",
    "def create_video_path_list():\n",
    "  video_path_options = [(\"None\", TRAINING_CONTENT_DIR_PATH)]  # path_changed from \"/content/training_content\"\n",
    "  for video_name in os.listdir(TRAINING_CONTENT_DIR_PATH):\n",
    "    video_path = os.path.join(TRAINING_CONTENT_DIR_PATH, video_name)\n",
    "    if os.path.isdir(video_path):\n",
    "      video_path_options.append((video_name, Path(f'{TRAINING_CONTENT_DIR_PATH}/{video_name}')))\n",
    "  return video_path_options\n",
    "\n",
    "# Check if the yaml is in the correct structure for inference\n",
    "def compare_dict_structure(expected, yaml_config):\n",
    "  for key, value in expected.items():\n",
    "    if not isinstance(value, dict):\n",
    "      if key not in yaml_config or not isinstance(yaml_config[key], value): # Check if key exists and if data type matches\n",
    "        if key in yaml_config and isinstance(list(yaml_config[key]), value): # Special check for list because omegaconf sees it as a ListConfig\n",
    "          return True\n",
    "        return False\n",
    "    elif key in yaml_config: # If value is a dict recurse to its first non dict value\n",
    "      compare_dict_structure(value, yaml_config[key])\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T07:59:28.034412Z",
     "start_time": "2023-10-28T07:59:28.022339Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Post-Process Inference Functions\n",
    "\n",
    "# Returns the -bg pose video\n",
    "def usePoseWithBG(skeleton_path):\n",
    "  video_name = skeleton_path.name\n",
    "  \n",
    "  new_path = Path(f'{skeleton_path.parents[0]}/.{video_name[:-4]}-bg.mp4')\n",
    "  \n",
    "  return new_path\n",
    "\n",
    "# Find the first occurence of a file name in video directory and its subdirs\n",
    "def findHumanFilePath(file_to_find, folder_path):\n",
    "  found_file = None\n",
    "\n",
    "  for item in os.listdir(folder_path):\n",
    "    if item == \"Skeleton\":\n",
    "      continue\n",
    "    if os.path.isfile(os.path.join(folder_path, item)):\n",
    "      if fnmatch.fnmatch(item, file_to_find):\n",
    "        found_file = os.path.join(folder_path, item)\n",
    "        return found_file\n",
    "    else:\n",
    "      found_file = findHumanFilePath(file_to_find, (folder_path / item))\n",
    "\n",
    "    if found_file:\n",
    "      break\n",
    "\n",
    "  return found_file\n",
    "\n",
    "# Combines all gifs with parameters to show superimposition and captions\n",
    "def postprocess_gif(inf_path, prompts, size, pose_type, show_caption, is_gif_superimposed):\n",
    "  # Create lists that will hold the path of input and videos to combine\n",
    "  gif_video_paths = []\n",
    "  clips = []\n",
    "\n",
    "  # Path containing the pose and the path of the output video\n",
    "  # Run the FFmpeg command to adjust the video's duration\n",
    "  pose_path = Path(f'{inf_path}/pose.gif')\n",
    "  if pose_type == \"human\":\n",
    "    pose_path = Path(f'{inf_path}/pose-human.gif')\n",
    "  elif pose_type == \"combination\":\n",
    "    pose_path = Path(f'{inf_path}/pose-superimposed.gif')\n",
    "\n",
    "  gif_output_path = Path(f'{inf_path}/processed/all_combined.gif')\n",
    "\n",
    "  # Check if only a single prompt was given\n",
    "  if isinstance(prompts, str):\n",
    "    prompts = [prompts]\n",
    "    gif_output_path = Path(f'{inf_path}/processed/{prompts[0]}.gif')\n",
    "\n",
    "  # Check if superimposed is true\n",
    "  if is_gif_superimposed:\n",
    "    gif_video_paths = [Path(f'{inf_path}/superimposed/{prompt}.gif') for prompt in prompts]\n",
    "  else:\n",
    "    gif_video_paths = [Path(f'{inf_path}/raw/{prompt}.gif') for prompt in prompts]\n",
    "\n",
    "  # Load video\n",
    "  skeleton_video = VideoFileClip(pose_path.as_posix())\n",
    "  for file_path in gif_video_paths:\n",
    "    gif_video = VideoFileClip(file_path.as_posix())\n",
    "    clips.append(gif_video)\n",
    "\n",
    "  # Video editting\n",
    "  skeleton_video = skeleton_video.resize((size, size))\n",
    "  clips = [video.resize((size, size)) for video in clips]\n",
    "  clips.insert(0, skeleton_video)\n",
    "\n",
    "  if not show_caption:\n",
    "    # Combine videos side by side and write to output path\n",
    "    result = clips_array([clips])\n",
    "    result.write_gif(gif_output_path, fps=10)\n",
    "\n",
    "  else:\n",
    "    # Create a black bar for captions\n",
    "    black_clip = ColorClip(size=(clips[0].w, clips[0].h + 40), color=(0, 0, 0), duration=clips[0].duration)\n",
    "    clips[0] = CompositeVideoClip([black_clip, clips[0]])\n",
    "\n",
    "    # Add captions\n",
    "    for index, prompt in enumerate(prompts):\n",
    "      txt_clip = TextClip(prompt, font=\"Amiri-bold\", fontsize=30, color='white')\n",
    "      txt_clip = txt_clip.set_duration(clips[index + 1].duration)\n",
    "      txt_clip = txt_clip.set_position((\"center\", \"bottom\"))\n",
    "\n",
    "      # Combine clips with captions\n",
    "      clips[index + 1] = CompositeVideoClip([black_clip, txt_clip, clips[index + 1]])\n",
    "\n",
    "    # Write the final GIF\n",
    "    gif_with_text = clips_array([clips])\n",
    "    gif_with_text.write_gif(gif_output_path, fps=10)\n",
    "\n",
    "# View the duration, number of frames and fps of a given gif\n",
    "def gif_stats(gif_path):\n",
    "  gif = imageio.get_reader(gif_path)\n",
    "\n",
    "  # Get the duration in seconds\n",
    "  duration = 0.0\n",
    "  for frame in gif:\n",
    "      duration += frame.meta['duration'] / 1000.0\n",
    "\n",
    "  fps = len(gif) / duration\n",
    "  print(f\"Duration: {duration}s, Frames: {len(gif)}, FPS: {fps}\")\n",
    "\n",
    "# Matches the skeleton video duration and fps to be the same as gif\n",
    "def postprocess_mmpose(skeleton_path, video_length, output_dir, pose_type):\n",
    "  \n",
    "  # Get the video duration using ffprobe\n",
    "  duration = float(subprocess.check_output(['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1', skeleton_path]))\n",
    "\n",
    "  # Set your desired duration\n",
    "  desired_duration = video_length * 130 / 1000\n",
    "\n",
    "  # Calculate the speedup factor\n",
    "  speedup_factor = desired_duration / duration\n",
    "\n",
    "  fps = video_length / desired_duration\n",
    "\n",
    "  # Run the FFmpeg command to adjust the video's duration\n",
    "  output_path = Path(f'{output_dir}/pose.gif')\n",
    "  if pose_type == \"human\":\n",
    "    output_path = Path(f'{output_dir}/pose-human.gif')\n",
    "  elif pose_type == \"combination\":\n",
    "    output_path = Path(f'{output_dir}/pose-superimposed.gif')\n",
    "\n",
    "  !ffmpeg -i $skeleton_path -vf \"setpts=$speedup_factor*PTS,fps=$fps\" -y $output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Browsing ðŸ‘€\n",
    "\n",
    "1. Play the Video Selection cell.\n",
    "2. Pick a folder.\n",
    "3. Pick a video.\n",
    "4. Click **Display** to view the video.\n",
    "5. The other buttons, **MMPose**, **FYP** and **Refresh** will be covered in the other sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T07:33:44.707202Z",
     "start_time": "2023-10-28T07:33:44.685212Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# @title Video Selection { display-mode: \"form\" }\n",
    "\n",
    "# Helper Functions\n",
    "def getFolderContent(folder_name):\n",
    "  subfolders = []\n",
    "  for content in os.listdir(VIDEO_DIR_PATH):\n",
    "        content_path = os.path.join(VIDEO_DIR_PATH, content)\n",
    "\n",
    "        # Check if it's a directory and not hidden\n",
    "        if os.path.isdir(content_path) and not content.startswith(\".\"):\n",
    "            subfolders.append(content)\n",
    "\n",
    "  return subfolders\n",
    "\n",
    "# Init\n",
    "video_dir_folders = os.listdir(VIDEO_DIR_PATH)\n",
    "video_dir_folder_content = getFolderContent(video_dir_folders)\n",
    "\n",
    "# Create layout\n",
    "layout_single_button = widgets.Layout(width='212px',margin='0px 0px 20px 90px')\n",
    "layout_double_button = widgets.Layout(width='104px')\n",
    "layout_hbox = widgets.Layout(margin='0px 0px 0px 88px')\n",
    "layout_output = widgets.Layout(margin='0px 0px 20px 0px', display='flex', align_items='flex-start')\n",
    "\n",
    "# Create widgets\n",
    "video_output_placeholder = widgets.Output(layout=layout_output)\n",
    "video_subdir_dropdown = widgets.Dropdown(options=video_dir_folder_content, description='Folder:', value=None, disabled=False)\n",
    "video_dropdown = widgets.Dropdown(options=[], description='Video:', disabled=True)\n",
    "video_display_button = widgets.Button(description=\"Display\", disabled=True, layout=layout_double_button)\n",
    "video_refresh_button = widgets.Button(description=\"Refresh\", disabled=False, layout=layout_double_button)\n",
    "video_mmpose_button = widgets.Button(description=\"MMPose\", disabled=True, layout=layout_double_button)\n",
    "video_fyp_button = widgets.Button(description=\"FYP\", disabled=True, layout=layout_double_button)\n",
    "\n",
    "video_hbox_1 = widgets.HBox([video_display_button, video_refresh_button], layout=layout_hbox)\n",
    "video_hbox_2 = widgets.HBox([video_mmpose_button, video_fyp_button], layout=layout_hbox)\n",
    "\n",
    "video_output_placeholder_content = HTML(\"\"\"\n",
    "  <div style=\"width: 512px; height: 512px; border-radius: 5%; background-color: black; margin: 0 auto; display: flex; justify-content: center; align-items: center;\">\n",
    "      <div style=\"width: 500px; height: 500px; border-radius: 5%; border: 2px solid white;\" />\n",
    "  </div>\n",
    "\"\"\")\n",
    "\n",
    "# Create listeners\n",
    "## Update video dropdown options based on the selected folder\n",
    "def video_subdir_select(change):\n",
    "    selected_video_folder = video_subdir_dropdown.value\n",
    "\n",
    "    if selected_video_folder != None:\n",
    "      selected_VIDEO_DIR_PATH = Path(f'{VIDEO_DIR_PATH}/{selected_video_folder}')\n",
    "      selected_video_dir_content = [file for file in os.listdir(selected_VIDEO_DIR_PATH) if file.endswith('.mp4')]\n",
    "    else:\n",
    "      selected_video_dir_content = []\n",
    "\n",
    "    video_dropdown.options = selected_video_dir_content\n",
    "    if not selected_video_dir_content:\n",
    "        video_dropdown.disabled = False\n",
    "        video_dropdown.value = None\n",
    "    else:\n",
    "        video_dropdown.disabled = False\n",
    "\n",
    "## Display the selected video\n",
    "def display_selected_video(change):\n",
    "    selected_video = video_dropdown.value\n",
    "    selected_video_folder = video_subdir_dropdown.value\n",
    "\n",
    "    if selected_video:\n",
    "        video_path = Path(f'{VIDEO_DIR_PATH}/{selected_video_folder}/{selected_video}')\n",
    "        video_display = Video(video_path, width=512, height=512, embed=True)\n",
    "\n",
    "        # Clear the output placeholder and display the video\n",
    "        with video_output_placeholder:\n",
    "            clear_output()\n",
    "            display(video_display)\n",
    "\n",
    "## Refresh folder and directory\n",
    "def refresh_folder_and_directory(change):\n",
    "    video_dir_folders = os.listdir(VIDEO_DIR_PATH)\n",
    "    video_dir_folder_content = getFolderContent(video_dir_folders)\n",
    "\n",
    "    video_dropdown.options = []\n",
    "    video_dropdown.value = None\n",
    "\n",
    "    video_subdir_dropdown.options = video_dir_folder_content\n",
    "    video_subdir_dropdown.value = None\n",
    "\n",
    "\n",
    "\n",
    "## Enable button when a valid video is picked\n",
    "def enable_button(change):\n",
    "    if video_dropdown.value:\n",
    "        video_display_button.disabled = False\n",
    "        video_mmpose_button.disabled = False\n",
    "        video_fyp_button.disabled = False\n",
    "    else:\n",
    "        video_display_button.disabled = True\n",
    "        video_mmpose_button.disabled = True\n",
    "        video_fyp_button.disabled = True\n",
    "\n",
    "# Attach Listeners\n",
    "video_subdir_dropdown.observe(video_subdir_select, 'value')\n",
    "video_display_button.on_click(display_selected_video)\n",
    "video_refresh_button.on_click(refresh_folder_and_directory)\n",
    "video_dropdown.observe(enable_button, 'value')\n",
    "\n",
    "\n",
    "# Display fields\n",
    "with video_output_placeholder:\n",
    "  display(video_output_placeholder_content)\n",
    "display(video_output_placeholder)\n",
    "display(video_subdir_dropdown)\n",
    "display(video_dropdown)\n",
    "\n",
    "display(video_hbox_1)\n",
    "display(video_hbox_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: MMPOSE ì›ƒ\n",
    "\n",
    "1. Play the Run MMPose Inference cell.\n",
    "2. Go back to the Video Selection cell and select a folder and a corresponding video and click **MMPose** . (Ensure the video is of a human doing any action)\n",
    "3. Wait for the **Video** box to be populated with the selected video.\n",
    "4. Click **Start Inference**.\n",
    "5. Done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T07:34:06.538170Z",
     "start_time": "2023-10-28T07:34:06.516093Z"
    }
   },
   "outputs": [],
   "source": [
    "# @title Run MMPose Inference { display-mode: \"form\" }\n",
    "\n",
    "# Helper Functions\n",
    "## Re-encode video due to H.264 video encoding error\n",
    "def reencode_video(input_file):\n",
    "  temp_output_file = Path(f'{VIDEO_SKELETON_DIR_PATH}/output.mp4')\n",
    "  !ffmpeg -i {input_file} -c:v libx264 -crf 23 -c:a aac -strict experimental {temp_output_file}\n",
    "\n",
    "  os.remove(input_file)\n",
    "  os.rename(temp_output_file, input_file)\n",
    "\n",
    "# Create layout\n",
    "layout_single_long_button = widgets.Layout(margin='0px 0px 20px 154px')\n",
    "\n",
    "# Create widgets\n",
    "selected_mmpose_video_input = widgets.Text(placeholder='Select a video above', description=\"Video:\", disabled=True)\n",
    "inf_mmpose_button = widgets.Button(description=\"Start Inference\", disabled=True, layout=layout_single_long_button)\n",
    "\n",
    "# Create listeners\n",
    "## Retrieve video input\n",
    "def update_mmpose_input_video(change):\n",
    "    selected_video = video_dropdown.value\n",
    "    selected_mmpose_video_input.value = selected_video\n",
    "    if selected_video:\n",
    "        inf_mmpose_button.disabled = False\n",
    "    else:\n",
    "        inf_mmpose_button.disabled = True\n",
    "\n",
    "## Run mmpose\n",
    "def run_mmpose_inference(button):\n",
    "    selected_video = video_dropdown.value\n",
    "    video_path = Path(f'{VIDEO_DIR_PATH}/{video_subdir_dropdown.value}/{selected_video}')\n",
    "\n",
    "    %cd -q {MMPOSE_DIR_PATH}\n",
    "\n",
    "    if not os.path.exists(VIDEO_SKELETON_DIR_PATH):\n",
    "        os.mkdir(VIDEO_SKELETON_DIR_PATH)\n",
    "\n",
    "    # Start inference with human background\n",
    "    !python demo/inferencer_demo.py \\\n",
    "        {video_path}  \\\n",
    "        --pose2d human \\\n",
    "        --vis-out-dir {VIDEO_SKELETON_DIR_PATH} \\\n",
    "        --thickness 4 \\\n",
    "        --radius 0\n",
    "\n",
    "    # Change name for human background\n",
    "    input_file = Path(f'{VIDEO_SKELETON_DIR_PATH}/{selected_video}')\n",
    "    bg_file = Path(f'{VIDEO_SKELETON_DIR_PATH}/.{selected_video[:-4]}-bg.mp4')\n",
    "\n",
    "    os.rename(input_file, bg_file)\n",
    "\n",
    "    # Start inference with black background\n",
    "    !python demo/inferencer_demo.py \\\n",
    "        {video_path}  \\\n",
    "        --pose2d human \\\n",
    "        --vis-out-dir {VIDEO_SKELETON_DIR_PATH} \\\n",
    "        --black-background \\\n",
    "        --thickness 4 \\\n",
    "        --radius 0\n",
    "\n",
    "\n",
    "    reencode_video(bg_file)\n",
    "    reencode_video(input_file)\n",
    "\n",
    "    clear_output()\n",
    "    mmpose_inf_display()\n",
    "    print(\"Done, Outputs:\")\n",
    "    print(f\"With Background: {bg_file}\")\n",
    "    print(f\"Black Background: {input_file}\")\n",
    "\n",
    "# Attach Listeners\n",
    "video_mmpose_button.on_click(update_mmpose_input_video)\n",
    "inf_mmpose_button.on_click(run_mmpose_inference)\n",
    "\n",
    "# Display fields\n",
    "def mmpose_inf_display():\n",
    "  display(selected_mmpose_video_input)\n",
    "  display(inf_mmpose_button)\n",
    "mmpose_inf_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: FYP ðŸ’ƒ\n",
    "\n",
    "1. Play the Inference Configuration cell.\n",
    "2. Update the configurations and **Save**. (Details for each field will be stated above the cell).\n",
    "3. Play the Run FYP Inference cell.\n",
    "4. Go back to the Video Selection cell click **Refresh**.\n",
    "5. Select the **Skeleton** folder and a video in that folder then click **FYP**.\n",
    "6. Once the Video box has been loaded, click **Start Inference** to begin the inference.\n",
    "7. Once inference is completed, play the Post-Process Inference MMPose cell.\n",
    "8. Play the Superimpose cell.\n",
    "9. Play the Combine Gif cell.\n",
    "10. Input all the fields and click **Create**. (Details for each field will be stated above the cell).\n",
    "11. Click the Gif Display cell.\n",
    "12. Choose a folder and a gif and click **Display**.\n",
    "13. Done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference Configuration** âš™ï¸\n",
    "\n",
    "**pretrained_model_path**: The path that contains the model to be used for inference.\n",
    "\n",
    "**output_dir**: The path where the inferred gifs are saved to. The box is for users to write the name of the inference folder.\n",
    "\n",
    "**Validation Data**:\n",
    "> **prompts**: A list of texts that the gifs will be generated based on.\n",
    ">\n",
    "> **video_length**: Number of frames referenced from the pose video.\n",
    ">\n",
    "> **width** and **height**: Resolution of the video.\n",
    ">\n",
    "> **num_inference_steps**: Higher the value the more relistic a video would be in exchange for higher memory usage, computational resouces and time spent to infer.\n",
    ">\n",
    "> **guidance_scale**: A scale used to control and predict noise.\n",
    ">\n",
    "> **use_inv_latent**: Whether to reverse engineer the process to determine the latent variables used to make up the real image. Unused in our current state.\n",
    ">\n",
    "> **num_inv_steps**: Adjust to optimize the inverse latent process.\n",
    ">\n",
    "> **dataset_set**: No need to be changed by the user.\n",
    "\n",
    "**train_batch_size** How much training can be done together at once. (Larger batch means faster training at the cost of higher memory usage)\n",
    "\n",
    "**resume_from_checkpoint**: The path that contains the checkpoint used for the model.\n",
    "\n",
    "**seed**: A set inference seed to limit and control randomness and ensure reproducibility in case of error and/or for debugging.\n",
    "\n",
    "**mixed_precision**: This is to set the type of precision for text encoding and VAE autoencoding weights. By default, this is set to single precision which is fp32. (High precision in exchange for more memory usage and computational resources used)\n",
    "\n",
    "**gradient_checkpointing**: Reduces memory usage by doing some checkpoints for gradients, which increases the computational load. Decreases memory usage for increased time taken for inference to complete.\n",
    "\n",
    "**enable_xformers_memory_efficient_attention**: Reduce memory usage in exchange for slight dip in inference performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T07:35:16.488494Z",
     "start_time": "2023-10-28T07:35:16.456407Z"
    }
   },
   "outputs": [],
   "source": [
    "# @title Inference Configuration { display-mode: \"form\" }\n",
    "\n",
    "# Init\n",
    "%cd -q {FYP_DIR_PATH}\n",
    "\n",
    "# Initialize load_config with a default value\n",
    "load_config = None\n",
    "\n",
    "# Define a container for displayed widgets\n",
    "displayed_widgets = []\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "config_files = [f for f in os.listdir(CONFIG_DIR_PATH) if os.path.isfile(os.path.join(CONFIG_DIR_PATH, f))]\n",
    "\n",
    "# Create a dropdown widget with the list of config files\n",
    "config_files_dropdown = Dropdown(\n",
    "  options=[\"- Select an Item -\"] + config_files,\n",
    "  description='Select a Config File:',\n",
    "  layout=Layout(width=\"500px\"),\n",
    "  style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "# Function to clear displayed widgets (excluding the dropdown)\n",
    "def clear_displayed_widgets():\n",
    "    for widget in displayed_widgets:\n",
    "        widget.close()\n",
    "    displayed_widgets.clear()\n",
    "    display(config_files_dropdown)  # Display the dropdown again\n",
    "\n",
    "# Function to update the load_config variable based on the selected filename\n",
    "def update_load_config(change):\n",
    "    global load_config\n",
    "    selected_filename = change.new\n",
    "    if selected_filename and selected_filename != \"- Select an Item -\":\n",
    "        clear_output(wait=True)  # Clear the output area\n",
    "        clear_displayed_widgets()  # Clear previously displayed widgets\n",
    "\n",
    "        # Load yaml file\n",
    "        sample_yaml_path = Path(f'{CONFIG_DIR_PATH}/{selected_filename}')\n",
    "        with open(sample_yaml_path, 'r') as yaml_file:\n",
    "          load_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "        print(f\"Editing config from: {sample_yaml_path}\")\n",
    "\n",
    "        # Check if the yaml configuration matches expected training config\n",
    "        if compare_dict_structure(expected_inference_config, load_config):\n",
    "\n",
    "          # Create a list of model names and paths in the directory\n",
    "          model_options = create_model_list()\n",
    "\n",
    "          # =====================================\n",
    "          ## Basic Data\n",
    "          config_subheader1 = widgets.HTML(value=\"<h3>Basic Data</h3>\")\n",
    "          config_pretrained_model_path = Dropdown(options=model_options, description=\"pretrained_model_path:\", value=Path(load_config['pretrained_model_path']), style=configs_config_style, layout=configs_config_layout)\n",
    "          config_output_dir_name = widgets.Text(description=\"output_dir_name:\", value=\"\", style=configs_config_style, layout=configs_config_layout)\n",
    "          config_train_batch_size = widgets.IntText(description=\"train_batch_size:\", value=load_config['train_batch_size'], style=configs_config_style, layout=configs_config_layout)\n",
    "          config_validation_steps = widgets.IntText(description=\"validation_steps:\", value=load_config['validation_steps'], style=configs_config_style, layout=configs_config_layout)\n",
    "          config_seed = widgets.IntText(description=\"seed:\", value=load_config['seed'], style=configs_config_style, layout=configs_config_layout)\n",
    "          config_mixed_precision = widgets.Text(description=\"mixed_precision:\", value=load_config['mixed_precision'], style=configs_config_style, layout=configs_config_layout)\n",
    "          config_gradient_checkpointing = Dropdown(options=boolean_dropdown, value=load_config['gradient_checkpointing'], description=\"gradient_checkpointing:\", style=configs_config_style, layout=configs_config_layout)\n",
    "          config_enable_xformers_memory_efficient_attention = Dropdown(options=boolean_dropdown, value=load_config['enable_xformers_memory_efficient_attention'], description=\"enable_xformers_memory_efficient_attention:\", style=configs_config_style, layout=configs_config_layout)\n",
    "          # =====================================\n",
    "          ## Validation_data\n",
    "          config_subheader2 = widgets.HTML(value=\"<h3>Validation Data</h3>\")\n",
    "          config_prompts = widgets.Textarea(description=\"prompts:\", value=\"\\n\".join(load_config['validation_data']['prompts']), style=configs_config_style, layout=widgets.Layout(width=\"500px\", height=\"100px\"))\n",
    "          config_video_length = widgets.IntText(description=\"video_length:\", value=load_config['validation_data']['video_length'], style=configs_config_style, layout=configs_config_layout)\n",
    "          config_width = widgets.IntText(description=\"width:\", value=load_config['validation_data']['width'], style=configs_config_style, layout=configs_config_layout)\n",
    "          config_height = widgets.IntText(description=\"height:\", value=load_config['validation_data']['height'], style=configs_config_style, layout=configs_config_layout)\n",
    "          config_num_inference_steps = widgets.IntText(description=\"num_inference_steps:\", value=load_config['validation_data']['num_inference_steps'], style=configs_config_style, layout=configs_config_layout)\n",
    "          config_guidance_scale = widgets.FloatText(description=\"guidance_scale:\", value=load_config['validation_data']['guidance_scale'], style=configs_config_style, layout=configs_config_layout)\n",
    "          config_use_inv_latent = Dropdown(options=boolean_dropdown, value=load_config['validation_data']['use_inv_latent'], description=\"use_inv_latent:\", style=configs_config_style, layout=configs_config_layout)\n",
    "          config_num_inv_steps = widgets.IntText(description=\"num_inv_steps:\", value=load_config['validation_data']['num_inv_steps'], style=configs_config_style, layout=configs_config_layout)\n",
    "          # =====================================\n",
    "          ## Group widgets\n",
    "          config_vbox = widgets.VBox([\n",
    "              config_pretrained_model_path,\n",
    "              config_output_dir_name,\n",
    "              config_train_batch_size,\n",
    "              config_validation_steps,\n",
    "              config_seed,\n",
    "              config_mixed_precision,\n",
    "              config_gradient_checkpointing,\n",
    "              config_enable_xformers_memory_efficient_attention,\n",
    "          ])\n",
    "          config_vbox_validation_data = widgets.VBox([\n",
    "              config_prompts,\n",
    "              config_video_length,\n",
    "              config_width,\n",
    "              config_height,\n",
    "              config_num_inference_steps,\n",
    "              config_guidance_scale,\n",
    "              config_use_inv_latent,\n",
    "              config_num_inv_steps\n",
    "          ])\n",
    "\n",
    "          # Create listeners\n",
    "          def save_config(change):\n",
    "            config = {\n",
    "                \"pretrained_model_path\": config_pretrained_model_path.value.as_posix(),\n",
    "                \"output_dir\": Path(f'{INFERENCE_OUTPUT_DIR_PATH}/{config_output_dir_name.value}').as_posix(),\n",
    "                \"validation_data\": {\n",
    "                    \"prompts\": [prompt.strip() for prompt in config_prompts.value.splitlines() if prompt.strip()],\n",
    "                    \"video_length\": config_video_length.value,\n",
    "                    \"width\": config_width.value,\n",
    "                    \"height\": config_height.value,\n",
    "                    \"num_inference_steps\": config_num_inference_steps.value,\n",
    "                    \"guidance_scale\": config_guidance_scale.value,\n",
    "                    \"use_inv_latent\": config_use_inv_latent.value,\n",
    "                    \"num_inv_steps\": config_num_inv_steps.value,\n",
    "                    \"dataset_set\": load_config['validation_data']['dataset_set']\n",
    "                },\n",
    "                \"train_batch_size\": config_train_batch_size.value,\n",
    "                \"validation_steps\": config_validation_steps.value,\n",
    "                \"resume_from_checkpoint\": load_config['resume_from_checkpoint'],\n",
    "                \"seed\": config_seed.value,\n",
    "                \"mixed_precision\": config_mixed_precision.value,\n",
    "                \"gradient_checkpointing\": config_gradient_checkpointing.value,\n",
    "                \"enable_xformers_memory_efficient_attention\": config_enable_xformers_memory_efficient_attention.value\n",
    "            }\n",
    "\n",
    "            # Specify the folder path you want to check\n",
    "            folder_path = Path(f'{INFERENCE_OUTPUT_DIR_PATH}/{config_output_dir_name.value}')\n",
    "\n",
    "            # Check if the folder exists\n",
    "            if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "              print(\"\\r\", f'There is already a folder with the name {config_output_dir_name.value}! Please rename your folder!', end=\"\")\n",
    "\n",
    "            else:\n",
    "              # Save updated config back into the yaml file\n",
    "              with open(sample_yaml_path, \"w\") as file:\n",
    "                yaml.dump(config, file, default_style='\"', default_flow_style=False, sort_keys=False)\n",
    "\n",
    "              print(\"\\r\", \"Saving...\", end=\"\")\n",
    "              time.sleep(2)\n",
    "              print(\"\\r\", \"Successfully saved!\", end=\"\")\n",
    "\n",
    "          ## Button Widget and Attach Listener\n",
    "          config_save_btn = widgets.Button(description=\"Save\", layout=configs_config_button_layout)\n",
    "          config_save_btn.on_click(save_config)\n",
    "\n",
    "          # Display fields (same as before)\n",
    "          display(\n",
    "            config_subheader1,\n",
    "            config_vbox,\n",
    "            config_subheader2,\n",
    "            config_vbox_validation_data,\n",
    "            config_save_btn\n",
    "          )\n",
    "\n",
    "          # Update the displayed_widgets list\n",
    "          displayed_widgets.extend([\n",
    "              config_subheader1,\n",
    "              config_vbox,\n",
    "              config_subheader2,\n",
    "              config_vbox_validation_data,\n",
    "              config_save_btn\n",
    "          ])\n",
    "\n",
    "        else:\n",
    "          print(\"The configuration for the yaml is not structured correctly for inference\")\n",
    "\n",
    "# Attach the event handler to the dropdown's 'value' trait\n",
    "config_files_dropdown.observe(update_load_config, names='value')\n",
    "\n",
    "# Display the dropdown widget and the output widget\n",
    "display(config_files_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T07:43:51.851836Z",
     "start_time": "2023-10-28T07:43:51.826488Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# @title Run FYP Inference { display-mode: \"form\" }\n",
    "\n",
    "# Create layout\n",
    "layout_single_long_button = widgets.Layout(margin='0px 0px 20px 154px')\n",
    "\n",
    "# Create widgets\n",
    "selected_fyp_video_input = widgets.Text(placeholder='Select a video above', description=\"Video:\", disabled=True)\n",
    "config_file_path = widgets.Text(value=Path(f'{CONFIG_DIR_PATH}/pose_sample_windows.yaml').as_posix(),description=\"Config File:\")\n",
    "inf_fyp_button = widgets.Button(description=\"Start Inference\", disabled=True, layout=layout_single_long_button)\n",
    "\n",
    "# Create listeners\n",
    "## Retrieve video input\n",
    "def update_fyp_input_video(change):\n",
    "    selected_video = video_dropdown.value\n",
    "    selected_fyp_video_input.value = Path(f'{VIDEO_DIR_PATH}/{video_subdir_dropdown.value}/{selected_video}').as_posix()\n",
    "    if selected_video:\n",
    "        inf_fyp_button.disabled = False\n",
    "    else:\n",
    "        inf_fyp_button.disabled = True\n",
    "\n",
    "## Run FYP\n",
    "def run_fyp_inference(button):\n",
    "    %cd -q {FYP_DIR_PATH}\n",
    "\n",
    "    config_file_path_text = config_file_path.value\n",
    "    video_file_path_text = selected_fyp_video_input.value\n",
    "\n",
    "    # Specify the folder path you want to check\n",
    "    load_config = OmegaConf.load(config_file_path_text)\n",
    "    folder_path = load_config.output_dir\n",
    "\n",
    "    # Check if the folder exists\n",
    "    if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "      print(\"\\r\", f'The folder path {load_config.output_dir}! Please rename your folder!', end=\"\")\n",
    "\n",
    "    else:\n",
    "      print(\"\\r\", \"\", end=\"\")\n",
    "      # Start inference\n",
    "      !accelerate launch txt2video.py \\\n",
    "          --config={config_file_path_text}  \\\n",
    "          --skeleton_path={video_file_path_text}\n",
    "\n",
    "## Pass on skeleton path\n",
    "def get_skeleton_path():\n",
    "    skeleton_path = Path(selected_fyp_video_input.value)\n",
    "    return skeleton_path\n",
    "\n",
    "## Pass on config path\n",
    "def get_config_path():\n",
    "    config_path = config_file_path.value\n",
    "    return config_path\n",
    "\n",
    "# Attach Listeners\n",
    "inf_fyp_button.on_click(run_fyp_inference)\n",
    "video_fyp_button.on_click(update_fyp_input_video)\n",
    "\n",
    "# Display fields\n",
    "display(selected_fyp_video_input)\n",
    "display(config_file_path)\n",
    "display(inf_fyp_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T07:58:51.152853Z",
     "start_time": "2023-10-28T07:58:51.091927Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Post-Inference MMPose\n",
    "\n",
    "# Load the inferred config file\n",
    "with open(f\"{CONFIG_DIR_PATH}/pose_sample_windows.yaml\", 'r') as yaml_file: # Use this if you copied the inference folder from gdrive and bypassed running inference\n",
    "# with open(get_config_path(), 'r') as yaml_file:\n",
    "  load_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "# Retrieve info needed from the config file\n",
    "video_length = load_config['validation_data']['video_length']\n",
    "output_dir = Path(load_config['output_dir'])\n",
    "\n",
    "# Proper getting skeleton\n",
    "# skeleton_path = get_skeleton_path()\n",
    "skeleton_path = Path('D:/code/repos/ict3104_team_05/video/Skeleton/0TM53.mp4') # Use this if you copied the inference folder from gdrive and bypassed running inference\n",
    "human_path = findHumanFilePath(skeleton_path.name, VIDEO_DIR_PATH)\n",
    "superimposed_path = usePoseWithBG(skeleton_path)\n",
    "\n",
    "print(skeleton_path)\n",
    "\n",
    "# Create a pose gif of all 3 types of pose (Human, Skeleton, Superimposed)\n",
    "postprocess_mmpose(skeleton_path, video_length, output_dir, \"pose\")\n",
    "postprocess_mmpose(human_path, video_length, output_dir, \"human\")\n",
    "postprocess_mmpose(superimposed_path, video_length, output_dir, \"combination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Superimpose\n",
    "\n",
    "def superimposeSkeleton(inf_folder_path):\n",
    "  raw_path = Path(f'{inf_folder_path}/raw')\n",
    "  skeleton = Path(f'{inf_folder_path}/pose.gif')\n",
    "\n",
    "  output_folder_path = Path(f'{inf_folder_path}/superimposed')\n",
    "\n",
    "  # Check if the superimposed folder exists\n",
    "  if not os.path.exists(output_folder_path):\n",
    "    # If it doesn't exist, create the superimposed folder\n",
    "    os.makedirs(output_folder_path)\n",
    "\n",
    "  # Get a list of all files in the directory\n",
    "  humans = [f for f in os.listdir(raw_path) if os.path.isfile(os.path.join(raw_path, f))]\n",
    "\n",
    "  for human_gif in humans:\n",
    "    if not human_gif.endswith(\".gif\"):\n",
    "      continue\n",
    "\n",
    "    human_path = Path(f'{raw_path}/{human_gif}')\n",
    "    output_file_path = Path(f'{output_folder_path}/{human_gif}')\n",
    "\n",
    "    # Load your two GIFs\n",
    "    fg = imageio.get_reader(skeleton)\n",
    "    bg = imageio.get_reader(human_path)\n",
    "\n",
    "    # Create a writer to save the result as a GIF\n",
    "    output_gif = imageio.get_writer(output_file_path, fps=7.692, loop=0)  # Adjust the desired frame rate\n",
    "\n",
    "    for i in range(min(len(fg), len(bg))):  # Process frames until one of the GIFs ends\n",
    "        foreground = fg.get_data(i)\n",
    "        background = bg.get_data(i)\n",
    "\n",
    "        foreground = cv2.resize(foreground, (512, 512))  # Resize to (480, 480)\n",
    "        background = cv2.resize(background, (512, 512))  # Resize to (480, 480)\n",
    "\n",
    "        # Creating the alpha mask from the foreground image (e.g., removing the black background)\n",
    "        gray = cv2.cvtColor(foreground, cv2.COLOR_BGR2GRAY)\n",
    "        foreground = foreground.astype(float)\n",
    "        background = background.astype(float)\n",
    "\n",
    "        # Dark pixels filter (0 to 255)\n",
    "        black_mask = (gray <= 50)\n",
    "\n",
    "        # Combine the images based on the mask\n",
    "        outImage = np.where(black_mask[:, :, np.newaxis], background, foreground)\n",
    "\n",
    "        # Convert the frame to uint8\n",
    "        ims = outImage.astype(np.uint8)\n",
    "\n",
    "        # Add the frame to the output GIF\n",
    "        output_gif.append_data(ims)\n",
    "\n",
    "    output_gif.close()\n",
    "    print('Done')\n",
    "\n",
    "\n",
    "# with open(f\"{CONFIG_DIR_PATH}/pose_sample.yaml\", 'r') as yaml_file: # Use this if you copied the inference folder from gdrive and bypassed running inference\n",
    "with open(get_config_path(), 'r') as yaml_file:\n",
    "  load_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "inf_folder_path = Path(load_config['output_dir'])\n",
    "superimposeSkeleton(inf_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Combine Gifs**\n",
    "\n",
    "**Inference Directory Name**: Shows a dropdown of folders in the inference output folder that the user can select.\n",
    "\n",
    "**Pose Type**: Shows 3 options as dropdown for user to select\n",
    "> **pose**: Skeleton pose video generated by MMPose with black background <br>\n",
    "> **human**: Original video <br>\n",
    "> **combination**: Skeleton pose superimposed onto the original video\n",
    "\n",
    "**Superimpose on Gif**: Boolean dropdown for user to indicate if they would like to see only the inferred gifs generated or the inferred gifs with the skeleton pose superimposed onto them.\n",
    "\n",
    "**Show Captions**: Boolean dropdown for user to indicate if they would like the prompt to be indicated for the inferred gif generated.\n",
    "\n",
    "**Combine all**: Boolean dropdown for user to indicate if they would like all the inferred gifs generated + the pose gif to be merged into 1 gif for display or separate it into individual inferred gif + skeleton pose gif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Combine Gifs\n",
    "\n",
    "\n",
    "\n",
    "def getInferenceRunFolderContent(folder_name):\n",
    "  runfolders = []\n",
    "  for content in os.listdir(INFERENCE_OUTPUT_DIR_PATH):\n",
    "        content_path = os.path.join(INFERENCE_OUTPUT_DIR_PATH, content)\n",
    "\n",
    "        # Check if it's a directory and not hidden\n",
    "        if os.path.isdir(content_path) and not content.startswith(\".\"):\n",
    "            runfolders.append(content)\n",
    "\n",
    "  return runfolders\n",
    "\n",
    "inference_run_folders = os.listdir(INFERENCE_OUTPUT_DIR_PATH)\n",
    "inference_run_folder_content = getInferenceRunFolderContent(inference_run_folders)\n",
    "\n",
    "# Configure layouts\n",
    "combine_gif_button_layout = widgets.Layout(margin='0px 0px 20px 210px', width=\"143px\")\n",
    "combine_gif_style = {'description_width': '200px'}\n",
    "combine_gif_layout = widgets.Layout(width=\"350px\")\n",
    "\n",
    "# Create widgets\n",
    "gif_inference_folder_name = Dropdown(options=inference_run_folder_content, description=\"Inference Directory Name:\", style=combine_gif_style, layout=combine_gif_layout)\n",
    "gif_pose_type = widgets.Dropdown(options=[\"pose\", \"human\", \"combination\"], description='Pose Type:', value=None, style=combine_gif_style, layout=combine_gif_layout)\n",
    "gif_inferred_type = widgets.Dropdown(options=boolean_dropdown, description='Superimpose on Gif:', value=None, style=combine_gif_style, layout=combine_gif_layout)\n",
    "gif_show_captions = widgets.Dropdown(options=boolean_dropdown, description='Show Captions:', value=None, style=combine_gif_style, layout=combine_gif_layout)\n",
    "gif_combine_all = widgets.Dropdown(options=boolean_dropdown, description='Combine all:', value=None, style=combine_gif_style, layout=combine_gif_layout)\n",
    "gif_create_button = widgets.Button(description=\"Create\", disabled=True, layout=combine_gif_button_layout)\n",
    "\n",
    "skeleton_path = get_skeleton_path()\n",
    "# skeleton_path = \"/content/video/Skeleton/0Tm53.mp4\" # Use this if you copied the inference folder from gdrive and bypassed running inference\n",
    "\n",
    "def clearTextOutput():\n",
    "  clear_output()\n",
    "\n",
    "  display(gif_inference_folder_name)\n",
    "  display(gif_pose_type)\n",
    "  display(gif_inferred_type)\n",
    "  display(gif_show_captions)\n",
    "  display(gif_combine_all)\n",
    "  display(gif_create_button)\n",
    "\n",
    "# Create listeners\n",
    "## Enable button when all dropdowns are populated\n",
    "def enable_button(change):\n",
    "    if gif_pose_type.value and gif_inferred_type.value is not None and gif_show_captions.value is not None and gif_combine_all.value is not None:\n",
    "      gif_create_button.disabled = False\n",
    "    else:\n",
    "      gif_create_button.disabled = True\n",
    "\n",
    "## Display the selected gif\n",
    "def combine_gifs(change):\n",
    "  gif_inf_folder = Path(f'{INFERENCE_OUTPUT_DIR_PATH}/{gif_inference_folder_name.value}')\n",
    "  pose_type = gif_pose_type.value\n",
    "  is_superimposed = gif_inferred_type.value\n",
    "  show_captions = gif_show_captions.value\n",
    "  combine_all = gif_combine_all.value\n",
    "\n",
    "  # Check if the folder exists\n",
    "  if os.path.exists(gif_inf_folder) and os.path.isdir(gif_inf_folder):\n",
    "    clearTextOutput()\n",
    "    # with open(f\"{CONFIG_DIR_PATH}/pose_sample.yaml\", 'r') as yaml_file: # Use this if you copied the inference folder from gdrive and bypassed running inference\n",
    "    with open(get_config_path(), 'r') as yaml_file:\n",
    "      load_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "    # Retrieve info needed from the config file\n",
    "    prompts = load_config['validation_data']['prompts']\n",
    "    size = load_config['validation_data']['width']\n",
    "\n",
    "    if combine_all:\n",
    "      postprocess_gif(gif_inf_folder, prompts, size, pose_type, show_captions, is_superimposed)\n",
    "    else:\n",
    "      [postprocess_gif(gif_inf_folder, prompt, size, pose_type, show_captions, is_superimposed) for prompt in prompts]\n",
    "    print(\"Done\")\n",
    "  else:\n",
    "    print(\"\\r\", 'No such folder exists!', end=\"\")\n",
    "\n",
    "# Attach Listeners\n",
    "gif_pose_type.observe(enable_button, names='value')\n",
    "gif_inferred_type.observe(enable_button, names='value')\n",
    "gif_show_captions.observe(enable_button, names='value')\n",
    "gif_combine_all.observe(enable_button, names='value')\n",
    "gif_create_button.on_click(combine_gifs)\n",
    "\n",
    "# Display fields\n",
    "display(gif_inference_folder_name)\n",
    "display(gif_pose_type)\n",
    "display(gif_inferred_type)\n",
    "display(gif_show_captions)\n",
    "display(gif_combine_all)\n",
    "display(gif_create_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Gif Display\n",
    "def getInferenceRunFolderContent(folder_name):\n",
    "  runfolders = []\n",
    "  for content in os.listdir(INFERENCE_OUTPUT_DIR_PATH):\n",
    "        content_path = os.path.join(INFERENCE_OUTPUT_DIR_PATH, content)\n",
    "\n",
    "        # Check if it's a directory and not hidden\n",
    "        if os.path.isdir(content_path) and not content.startswith(\".\"):\n",
    "            runfolders.append(content)\n",
    "\n",
    "  return runfolders\n",
    "\n",
    "# Init\n",
    "inference_run_folders = os.listdir(INFERENCE_OUTPUT_DIR_PATH)\n",
    "inference_run_folder_content = getInferenceRunFolderContent(inference_run_folders)\n",
    "\n",
    "# Create widgets\n",
    "gif_output_placeholder = widgets.Output(layout=layout_output)\n",
    "gif_subdir_dropdown = widgets.Dropdown(options=inference_run_folder_content, description='Folder:', value=None, disabled=False)\n",
    "gif_dropdown = widgets.Dropdown(options=[], description='Gif:', disabled=True)\n",
    "gif_display_button = widgets.Button(description=\"Display\", disabled=True, layout=layout_double_button)\n",
    "gif_refresh_button = widgets.Button(description=\"Refresh\", disabled=False, layout=layout_double_button)\n",
    "\n",
    "gif_hbox_1 = widgets.HBox([gif_display_button, gif_refresh_button], layout=layout_hbox)\n",
    "\n",
    "# Create listeners\n",
    "## Update gif dropdown options based on the selected folder\n",
    "def gif_subdir_select(change):\n",
    "    selected_gif_folder = gif_subdir_dropdown.value\n",
    "\n",
    "    if selected_gif_folder != None:\n",
    "      selected_GIF_DIR_PATH = Path(f'{INFERENCE_OUTPUT_DIR_PATH}/{selected_gif_folder}/processed')\n",
    "      selected_gif_dir_content = [file for file in os.listdir(selected_GIF_DIR_PATH) if file.endswith('.gif')]\n",
    "    else:\n",
    "      selected_gif_dir_content = []\n",
    "\n",
    "    gif_dropdown.options = selected_gif_dir_content\n",
    "    if not selected_gif_dir_content:\n",
    "        gif_dropdown.disabled = False\n",
    "        gif_dropdown.value = None\n",
    "    else:\n",
    "        gif_dropdown.disabled = False\n",
    "\n",
    "## Display the selected gif\n",
    "def display_selected_gif(change):\n",
    "    selected_gif = gif_dropdown.value\n",
    "    selected_gif_folder = gif_subdir_dropdown.value\n",
    "\n",
    "    if selected_gif:\n",
    "        gif_path = Path(f'{INFERENCE_OUTPUT_DIR_PATH}/{selected_gif_folder}/processed/{selected_gif}')\n",
    "        gif_display = Image(filename=gif_path, embed=True)\n",
    "\n",
    "        # Clear the output placeholder and display the gif\n",
    "        with gif_output_placeholder:\n",
    "            clear_output()\n",
    "            display(gif_display)\n",
    "\n",
    "## Refresh folder and directory\n",
    "def refresh_folder_and_directory(change):\n",
    "    inference_run_folders = os.listdir(INFERENCE_OUTPUT_DIR_PATH)\n",
    "    inference_run_folder_content = getInferenceRunFolderContent(inference_run_folders)\n",
    "\n",
    "    gif_dropdown.options = []\n",
    "    gif_dropdown.value = None\n",
    "\n",
    "    gif_subdir_dropdown.options = inference_run_folder_content\n",
    "    gif_subdir_dropdown.value = None\n",
    "\n",
    "\n",
    "## Enable button when a valid gif is picked\n",
    "def enable_button(change):\n",
    "    if gif_dropdown.value:\n",
    "        gif_display_button.disabled = False\n",
    "    else:\n",
    "        gif_display_button.disabled = True\n",
    "\n",
    "# Attach Listeners\n",
    "gif_subdir_dropdown.observe(gif_subdir_select, 'value')\n",
    "gif_display_button.on_click(display_selected_gif)\n",
    "gif_refresh_button.on_click(refresh_folder_and_directory)\n",
    "gif_dropdown.observe(enable_button, 'value')\n",
    "\n",
    "\n",
    "# Display fields\n",
    "display(gif_output_placeholder)\n",
    "display(gif_subdir_dropdown)\n",
    "display(gif_dropdown)\n",
    "\n",
    "display(gif_hbox_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ðŸ‹\n",
    "\n",
    "1. Play the Init cell.\n",
    "2. Play the Dataset Preload (Video) cell.\n",
    "3. Select a dataset folder and click **Start Cutting**.\n",
    "4. Play Dataset Preload (Metadata) cell.\n",
    "5. Play the Training Configuration cell.\n",
    "6. Update the configurations and click **Save**. (Details for each field will be shown above the cell).\n",
    "7. Play Run Training cell.\n",
    "8. Done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Init { display-mode: \"form\" }\n",
    "\n",
    "# Charades Data Class from csv\n",
    "class CharadesData:\n",
    "  def __init__(self, row):\n",
    "    id, subject, scene, quality, relevance, verified, script, objects, descriptions, actions, length = row\n",
    "    self.id = id\n",
    "    self.subject = subject\n",
    "    self.scene = scene\n",
    "    self.quality = quality\n",
    "    self.relevance = relevance\n",
    "    self.verified = verified\n",
    "    self.script = script\n",
    "    self.objects = objects.split(\";\")\n",
    "    self.descriptions = descriptions\n",
    "    self.length = length\n",
    "    self.actions = {}\n",
    "\n",
    "    # Convert actions in proper data structure (\"class_id time_start time_end\" -> class_id: [time_start, time_end])\n",
    "    if len(actions) != 0:\n",
    "      action_substrings = actions.split(';')\n",
    "      for substring in action_substrings:\n",
    "        parts = substring.split()\n",
    "        key = parts[0]\n",
    "        values = [self.convert_to_ms(parts[1]), self.convert_to_ms(parts[2])]\n",
    "        self.actions[key] = values\n",
    "\n",
    "  # For printing\n",
    "  def __str__(self):\n",
    "        return f\"ID: {self.id}, Subject: {self.subject}, Scene: {self.scene}, Quality: {self.quality}, Relevance: {self.relevance}, Verified: {self.verified}, Script: {self.script}, Objects: {self.objects}, Descriptions: {self.descriptions}, Actions: {self.actions}, Length: {self.length}\"\n",
    "\n",
    "  # Helper function to convert time into ms\n",
    "  def convert_to_ms(self, seconds):\n",
    "    ss,ms = seconds.split('.')\n",
    "    total_ms = 1000*int(ss) + int(ms)\n",
    "    return total_ms\n",
    "\n",
    "  # Caption getter with template\n",
    "  def getCaption(self, index):\n",
    "    return f\"In a {self.scene} setting, within the context of '{self.script}', the action '{action_descriptions[list(self.actions.keys())[index]]}' is taking place.\"\n",
    "\n",
    "\n",
    "action_descriptions = {}\n",
    "charades_all = []\n",
    "\n",
    "# Load classes lookup table\n",
    "with open(Path(f'{CHARADES_LOOKUP_PATH}/Charades_v1_classes.txt'), 'r') as file:\n",
    "    for line in file:\n",
    "        code, description = line.strip().split(' ', 1)\n",
    "        action_descriptions[code] = description\n",
    "\n",
    "# Load charades data A\n",
    "with open(Path(f'{CHARADES_LOOKUP_PATH}/Charades_v1_train.csv'), mode='r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    next(csv_reader, None)\n",
    "\n",
    "    for row in csv_reader:\n",
    "        charadeData = CharadesData(row)\n",
    "        charades_all.append(charadeData)\n",
    "\n",
    "# Load charades data B\n",
    "with open(Path(f'{CHARADES_LOOKUP_PATH}/Charades_v1_test.csv'), mode='r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    next(csv_reader, None)\n",
    "\n",
    "    for row in csv_reader:\n",
    "        charadeData = CharadesData(row)\n",
    "        charades_all.append(charadeData)\n",
    "\n",
    "clear_output()\n",
    "print(\"Data load successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Dataset Preload (Video) { display-mode: \"form\" }\n",
    "\n",
    "# Init\n",
    "dataset_dir_folders = os.listdir(VIDEO_DIR_PATH)\n",
    "\n",
    "## Training env\n",
    "training_dataset = None\n",
    "training_idx = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "training_branch = Path(f'{TRAINING_CONTENT_DIR_PATH}/{training_idx}')\n",
    "\n",
    "# Create layout\n",
    "config_style = {'description_width': '100px'}\n",
    "config_layout = widgets.Layout(width=\"300px\")\n",
    "config_button_layout = widgets.Layout(margin='0px 0px 20px 154px')\n",
    "\n",
    "# Create widgets\n",
    "dataset_folder_dropdown = widgets.Dropdown(options=[dir for dir in dataset_dir_folders if not dir.startswith(\".\")], description='Dataset Folder:', value=None, layout=config_layout, style=config_style)\n",
    "dataset_cutting_button = widgets.Button(description=\"Start Cutting\", disabled=True, layout=config_button_layout)\n",
    "\n",
    "# Create listeners\n",
    "def dataset_dir_select(change):\n",
    "  if dataset_folder_dropdown:\n",
    "    dataset_folder_content = os.listdir(Path(f'{VIDEO_DIR_PATH}/{dataset_folder_dropdown.value}'))\n",
    "    total_dataset_videos = len([file for file in dataset_folder_content if file.endswith('.mp4')])\n",
    "\n",
    "    dataset_cutting_button.disabled = False\n",
    "    dataset_cutting_button.description = f\"Start Cutting ({total_dataset_videos})\"\n",
    "\n",
    "def video_cutting_select(change):\n",
    "  os.makedirs(training_branch, exist_ok=True)\n",
    "  training_dataset = Path(f'{VIDEO_DIR_PATH}/{dataset_folder_dropdown.value}')\n",
    "  \n",
    "  # Loop video files from selected dataset folder\n",
    "  for video_file in os.listdir(training_dataset):\n",
    "    \n",
    "    video, ext = os.path.splitext(video_file)\n",
    "\n",
    "    # Ignore non video files (Eg: .ipynb_checkpoint and csv)\n",
    "    if ext != \".mp4\":\n",
    "      continue\n",
    "\n",
    "    video_folder = Path(f'{training_branch}/{video}')\n",
    "    if not os.path.exists(video_folder):\n",
    "      os.mkdir(video_folder)\n",
    "\n",
    "      # Retrieve charade object by ID\n",
    "      charade_data = None\n",
    "      \n",
    "      for charade in charades_all:\n",
    "        if charade.id == video:\n",
    "            charade_data = charade\n",
    "            break\n",
    "        \n",
    "      # If no clipping required, keep whole video\n",
    "      if not charade_data.actions:\n",
    "        print(f\"No clipping needed\")\n",
    "      else:\n",
    "        print(f\"Clipping {video}\")\n",
    "        charade_actions = charade_data.actions.items()\n",
    "        total_charade_actions = len(charade_actions)\n",
    "        for i, (class_id, timings) in enumerate(charade_actions):\n",
    "\n",
    "          input_video = Path(f'{training_dataset}/{video_file}')\n",
    "          output_video = Path(f'{video_folder}/{video}{i+1:02}{ext}')\n",
    "\n",
    "          print(f\"#{i+1}/{total_charade_actions}: {timings[0]}ms to {timings[0]+timings[1]}ms [I:{input_video}] [O:{output_video}]\")\n",
    "          !ffmpeg -i {input_video} -ss {timings[0]}ms -t {timings[1]}ms -c:v libx264 -c:a aac {output_video} -loglevel quiet\n",
    "    else:\n",
    "      print(f\"Folder already exist for video_id: {video}. Skipping ...\")\n",
    "  print(\"Finished Clipping\")\n",
    "\n",
    "# Attach Listeners\n",
    "dataset_folder_dropdown.observe(dataset_dir_select, 'value')\n",
    "dataset_cutting_button.on_click(video_cutting_select)\n",
    "\n",
    "# Display fields\n",
    "display(dataset_folder_dropdown)\n",
    "display(dataset_cutting_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Dataset Preload (Metadata) { display-mode: \"form\" }\n",
    "\n",
    "training_metadata_file = Path(f'{training_branch}/metadata.tsv')\n",
    "with open(training_metadata_file, 'w', newline='', encoding='utf-8') as tsvfile:\n",
    "  fieldnames = ['part_id', 'clip_id', 'caption']\n",
    "  writer = csv.DictWriter(tsvfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "  writer.writeheader()\n",
    "\n",
    "  for part_id in os.listdir(training_branch):\n",
    "    folder_path = os.path.join(training_branch, part_id)\n",
    "\n",
    "    # Ignore non video files (Eg: .ipynb_checkpoint and csv)\n",
    "    if not os.path.isdir(folder_path) or part_id.startswith(\".\"):\n",
    "      continue\n",
    "\n",
    "    charade_data = None\n",
    "    for charade in charades_all:\n",
    "      if charade.id == part_id:\n",
    "          charade_data = charade\n",
    "          break\n",
    "\n",
    "    if not charade_data:\n",
    "      print(\"Missing charades data, skipping ...\")\n",
    "      continue\n",
    "\n",
    "    # Sort by video sub-id to maintain order\n",
    "    training_video_files = sorted(os.listdir(folder_path), key=lambda x: int(os.path.splitext(x)[0][-2:]))\n",
    "    for i, clip in enumerate(training_video_files):\n",
    "      caption = charade_data.getCaption(i)\n",
    "      writer.writerow({\n",
    "          'part_id': part_id,\n",
    "          'clip_id': clip,\n",
    "          'caption': caption\n",
    "      })\n",
    "print(f\"TSV created: {training_metadata_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training Configuration** âš™ï¸\n",
    "\n",
    "**pretrained_model_path**: The path that contains the model to fine-tune. This will be a dropdown for the user to select from.\n",
    "\n",
    "**output_dir**: The path where the newly fine tuned model is pushed to. The folder name itself is written by the user, whereas the path to the folder is currently fixed.\n",
    "\n",
    "**Train Data**:\n",
    "> **video_path**: The path that contains the training dataset.\n",
    ">\n",
    "> **n_sample_frames**: Determines how many frames are referenced for training.\n",
    ">\n",
    "> **width**: Resolution of the video.\n",
    ">\n",
    "> **sample_frame_rate**: The rate at which the frames are sampled from. If the sample frames are set to 10 and the frame rate is set to 2, then every second 2 frames are referenced.\n",
    "\n",
    "**learning_rate** The rate at which each step of the training is conducted.\n",
    "\n",
    "**train_batch_size**: How much training can be done together at once. (Larger batch means faster training at the cost of higher memory usage)\n",
    "\n",
    "**max_train_steps**: The number of iterations the dataset is ran to optimize training.\n",
    "\n",
    "**trainable_modules**: The modules that are being trained (No change to be made be a user as the training state would be the same unless requirements change)\n",
    "\n",
    "**seed**: A set training seed to limit and control randomness and ensure reproducibility in case of error and/or for debugging.\n",
    "\n",
    "**mixed_precision**: This is to set the type of precision for text encoding and VAE autoencoding weights. By default, this is set to single precision which is fp32. (High precision in exchange for more memory usage and computational resources used)\n",
    "\n",
    "**use_8bit_adam**: Can be toggled true to reduce memory usage and computational resources used by using 8 bit precision for some part of ADAM optimization computations.\n",
    "\n",
    "**gradient_checkpointing**: Reduces memory usage by doing some checkpoints for gradients, which increases the computational load. Decreases memory usage for increased time taken for training completion.\n",
    "\n",
    "**enable_xformers_memory_efficient_attention**: Reduce memory usage in exchange for slight dip in training performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Training Configuration { display-mode: \"form\" }\n",
    "\n",
    "# Init\n",
    "%cd -q {FYP_DIR_PATH}\n",
    "\n",
    "# Initialize load_config with a default value\n",
    "load_config = None\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "config_files = [f for f in os.listdir(CONFIG_DIR_PATH) if os.path.isfile(os.path.join(CONFIG_DIR_PATH, f))]\n",
    "\n",
    "# Define a container for displayed widgets\n",
    "displayed_widgets = []\n",
    "\n",
    "# Create a dropdown widget with the list of config files\n",
    "config_files_dropdown = Dropdown(\n",
    "  options=[\"- Select an Item -\"] + config_files,\n",
    "  description='Select a Config File:',\n",
    "  layout=Layout(width=\"500px\"),\n",
    "  style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "# Function to clear displayed widgets (excluding the dropdown)\n",
    "def clear_displayed_widgets():\n",
    "    for widget in displayed_widgets:\n",
    "        widget.close()\n",
    "    displayed_widgets.clear()\n",
    "    display(config_files_dropdown)  # Display the dropdown again\n",
    "\n",
    "# Function to update the load_config variable based on the selected filename\n",
    "def update_load_config(change):\n",
    "    global load_config\n",
    "    selected_filename = change.new\n",
    "    if selected_filename and selected_filename != \"- Select an Item -\":\n",
    "        clear_output(wait=True)  # Clear the output area\n",
    "        clear_displayed_widgets()  # Clear previously displayed widgets\n",
    "\n",
    "        # Load yaml file\n",
    "        sample_yaml_path = Path(f'{CONFIG_DIR_PATH}/{selected_filename}')\n",
    "        # load_config = OmegaConf.load(sample_yaml_path)\n",
    "        with open(sample_yaml_path, 'r') as yaml_file:\n",
    "          load_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "        print(f\"Editing config from: {sample_yaml_path}\")\n",
    "\n",
    "        # Check if the yaml configuration matches expected training config\n",
    "        if compare_dict_structure(expected_training_config, load_config):\n",
    "\n",
    "          # Create a list of model names and paths in the directory\n",
    "          model_options = create_model_list()\n",
    "          video_options = create_video_path_list()\n",
    "          video_path_default = load_config[\"train_data\"][\"video_path\"]\n",
    "\n",
    "          if not (os.path.exists(video_path_default) and os.path.isdir(video_path_default)):\n",
    "            video_path_default = Path(TRAINING_CONTENT_DIR_PATH)\n",
    "          # =====================================\n",
    "          ## Basic Data\n",
    "          config_subheader1 = widgets.HTML(value=\"<h3>Basic Data</h3>\")\n",
    "          config_pretrained_model_path = Dropdown(options=model_options, description=\"pretrained_model_path:\", value=Path(load_config[\"pretrained_model_path\"]), style=configs_config_style, layout=configs_config_layout)\n",
    "          config_output_dir_name = widgets.Text(description=\"output_dir_name:\", value=\"\", style=configs_config_style, layout=configs_config_layout)\n",
    "          config_learning_rate = widgets.FloatText(description=\"learning_rate:\", value=load_config[\"learning_rate\"], style=configs_config_style, layout=configs_config_layout)\n",
    "          # =====================================\n",
    "          # train_data\n",
    "          config_subheader2 = widgets.HTML(value=\"<h3>Train Data</h3>\")\n",
    "          config_video_path = Dropdown(options=video_options, description=\"video_path:\", value=video_path_default, style=configs_config_style, layout=configs_config_layout)\n",
    "          config_n_sample_frames = widgets.IntText(description=\"n_sample_frames:\", value=load_config[\"train_data\"][\"n_sample_frames\"], style=configs_config_style, layout=configs_config_layout)\n",
    "          config_train_data_width = widgets.IntText(description=\"width:\", value=load_config[\"train_data\"][\"width\"], style=configs_config_style, layout=configs_config_layout)\n",
    "          config_sample_frame_rate = widgets.IntText(description=\"sample_frame_rate:\", value=load_config[\"train_data\"][\"sample_frame_rate\"], style=configs_config_style, layout=configs_config_layout)\n",
    "          # =====================================\n",
    "          config_train_batch_size = widgets.IntText(description=\"train_batch_size:\", value=load_config[\"train_batch_size\"], style=configs_config_style, layout=configs_config_layout)\n",
    "          config_max_train_steps = widgets.IntText(description=\"max_train_steps:\", value=load_config[\"max_train_steps\"], style=configs_config_style, layout=configs_config_layout)\n",
    "          config_seed = widgets.IntText(description=\"seed:\", value=load_config[\"seed\"], style=configs_config_style, layout=configs_config_layout)\n",
    "          config_mixed_precision = widgets.Text(description=\"mixed_precision:\", value=load_config[\"mixed_precision\"], style=configs_config_style, layout=configs_config_layout)\n",
    "          config_use_8bit_adam = Dropdown(options=boolean_dropdown, value=load_config[\"use_8bit_adam\"], description=\"config_use_8bit_adam:\", style=configs_config_style, layout=configs_config_layout)\n",
    "          config_gradient_checkpointing = Dropdown(options=boolean_dropdown, value=load_config[\"gradient_checkpointing\"], description=\"gradient_checkpointing:\", style=configs_config_style, layout=configs_config_layout)\n",
    "          config_enable_xformers_memory_efficient_attention = Dropdown(options=boolean_dropdown, value=load_config[\"enable_xformers_memory_efficient_attention\"], description=\"enable_xformers_memory_efficient_attention:\", style=configs_config_style, layout=configs_config_layout)\n",
    "          # =====================================\n",
    "          ## Button Widget\n",
    "          config_save_btn = widgets.Button(description=\"Save\", layout=configs_config_button_layout)\n",
    "          ## Group widgets\n",
    "          config_vbox = widgets.VBox([\n",
    "              config_pretrained_model_path,\n",
    "              config_output_dir_name,\n",
    "              config_learning_rate,\n",
    "              config_train_batch_size,\n",
    "              config_max_train_steps,\n",
    "              config_seed,\n",
    "              config_mixed_precision,\n",
    "              config_use_8bit_adam,\n",
    "              config_gradient_checkpointing,\n",
    "              config_enable_xformers_memory_efficient_attention\n",
    "          ])\n",
    "          config_vbox_train_data = widgets.VBox([\n",
    "              config_video_path,\n",
    "              config_n_sample_frames,\n",
    "              config_train_data_width,\n",
    "              config_sample_frame_rate,\n",
    "          ])\n",
    "\n",
    "          # Display fields (same as before)\n",
    "          display(\n",
    "            config_subheader1,\n",
    "            config_vbox,\n",
    "            config_subheader2,\n",
    "            config_vbox_train_data,\n",
    "            config_save_btn\n",
    "          )\n",
    "\n",
    "          # Create listeners\n",
    "          def save_config(change):\n",
    "            config = {\n",
    "                \"pretrained_model_path\": Path(config_pretrained_model_path.value).as_posix(),\n",
    "                \"output_dir\": Path(f\"{CUSTOM_MODEL_DIR_PATH}/{config_output_dir_name.value}\").as_posix(),\n",
    "                \"train_data\": {\n",
    "                    \"video_path\": Path(config_video_path.value).as_posix(),\n",
    "                    \"n_sample_frames\": config_n_sample_frames.value,\n",
    "                    \"width\": config_train_data_width.value,\n",
    "                    \"sample_frame_rate\": config_sample_frame_rate.value\n",
    "                },\n",
    "                \"learning_rate\": config_learning_rate.value,\n",
    "                \"train_batch_size\": config_train_batch_size.value,\n",
    "                \"max_train_steps\": config_max_train_steps.value,\n",
    "                \"trainable_modules\": load_config[\"trainable_modules\"],\n",
    "                \"seed\": config_seed.value,\n",
    "                \"mixed_precision\": config_mixed_precision.value,\n",
    "                \"use_8bit_adam\": config_use_8bit_adam.value,\n",
    "                \"gradient_checkpointing\": config_gradient_checkpointing.value,\n",
    "                \"enable_xformers_memory_efficient_attention\": config_enable_xformers_memory_efficient_attention.value\n",
    "            }\n",
    "\n",
    "            if Path(config_video_path.value) == Path(f'{TRAINING_CONTENT_DIR_PATH}/'):\n",
    "              print(\"\\r\", \"Please choose a training folder!\", end=\"\")\n",
    "\n",
    "            else:\n",
    "              #Save updated config back into yaml file\n",
    "              with open(sample_yaml_path, \"w\") as file:\n",
    "                yaml.dump(config, file, default_style='\"', default_flow_style=False, sort_keys=False)\n",
    "\n",
    "              print(\"\\r\", \"Saving...\", end=\"\")\n",
    "              time.sleep(2)\n",
    "              print(\"\\r\", \"Successfully saved!\", end=\"\")\n",
    "\n",
    "          # Attach Listeners\n",
    "          config_save_btn.on_click(save_config)\n",
    "\n",
    "          # Update the displayed_widgets list\n",
    "          displayed_widgets.extend([\n",
    "              config_subheader1,\n",
    "              config_vbox,\n",
    "              config_subheader2,\n",
    "              config_vbox_train_data,\n",
    "              config_save_btn,\n",
    "          ])\n",
    "\n",
    "        else:\n",
    "          print(\"The configuration for the yaml is not structured correctly for training\")\n",
    "\n",
    "# Attach the event handler to the dropdown's 'value' trait\n",
    "config_files_dropdown.observe(update_load_config, names='value')\n",
    "\n",
    "# Display the dropdown widgets\n",
    "display(config_files_dropdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Perform Training** â–¶ï¸\n",
    "\n",
    "1. Running the code would immediately start the training process.\n",
    "\n",
    "2. Once the training process starts, there will be a 2 minute buffer to load the necessary data for training.\n",
    "\n",
    "3. After the buffer, a progress bar would show displaying the progress of the training together with the percentage of completion.\n",
    "\n",
    "4. Finally when the training is completed, the newly generated model will be saved in the output directory path set by the user in the Training Configuration section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run Training { display-mode: \"form\" }\n",
    "\n",
    "%cd -q {FYP_DIR_PATH}\n",
    "\n",
    "output_label = widgets.Label(value=\"Output will appear here:\")\n",
    "display(output_label)\n",
    "\n",
    "def run_command_and_display_output(command):\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True, shell=True)\n",
    "    for line in process.stdout:\n",
    "        output_label.value = line.strip()  # Update the label with the live output\n",
    "    process.wait()\n",
    "\n",
    "run_command_and_display_output('accelerate launch train_followyourpose.py --config=\"configs/pose_train_windows.yaml\"')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
